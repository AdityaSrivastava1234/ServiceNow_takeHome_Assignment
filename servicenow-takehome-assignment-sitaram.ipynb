{"metadata":{"colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"5f7f0ccfc8d7413192d0bfe16a345ba2":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_b4828e7e2ab545ae9617f8d73dfa7117","IPY_MODEL_bd242b09a77b46f2bc671f164da6e27e","IPY_MODEL_8dbab1308ba048f7bfd1112709433350"],"layout":"IPY_MODEL_98ae9f32c1ff479abe7090f3e5627514"}},"b4828e7e2ab545ae9617f8d73dfa7117":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_2fa307bd7abf4797bc97c37660764c63","placeholder":"​","style":"IPY_MODEL_db93551c4a4245c2a00fb5e748295e4b","value":"spiece.model: 100%"}},"bd242b09a77b46f2bc671f164da6e27e":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_7c2f2245c5794a8382075c821fbe14c9","max":791656,"min":0,"orientation":"horizontal","style":"IPY_MODEL_e5d8bfb8b1224642a169176d829e7c01","value":791656}},"8dbab1308ba048f7bfd1112709433350":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_0e5801064ded473080e793bccc1d6ce2","placeholder":"​","style":"IPY_MODEL_2241b588baf041f081ccf21e955cce3b","value":" 792k/792k [00:00&lt;00:00, 3.05MB/s]"}},"98ae9f32c1ff479abe7090f3e5627514":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2fa307bd7abf4797bc97c37660764c63":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"db93551c4a4245c2a00fb5e748295e4b":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"7c2f2245c5794a8382075c821fbe14c9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e5d8bfb8b1224642a169176d829e7c01":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"0e5801064ded473080e793bccc1d6ce2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2241b588baf041f081ccf21e955cce3b":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"c71c0cf80c924c209c969598dfd5272e":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_8d56c9cfa8044d2d8ca8195db76e22d7","IPY_MODEL_d769cdbd8da5401d9c08bd2bb1ddfd0d","IPY_MODEL_ecb77b7e9adb40a7a197cc5c955192a8"],"layout":"IPY_MODEL_f74da5470f84455bb391084713d612c7"}},"8d56c9cfa8044d2d8ca8195db76e22d7":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e298faf220814043bc6d3b61fca7922c","placeholder":"​","style":"IPY_MODEL_dd6641a848324bd9a11b1115a2490358","value":"tokenizer.json: 100%"}},"d769cdbd8da5401d9c08bd2bb1ddfd0d":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_5dd7feecefe4427ea642a09ed44582df","max":1389353,"min":0,"orientation":"horizontal","style":"IPY_MODEL_768bf865f51b4361ab54a71a7f575a8c","value":1389353}},"ecb77b7e9adb40a7a197cc5c955192a8":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_fbeddacd9621481ea4f887b2b3cbd1aa","placeholder":"​","style":"IPY_MODEL_557741c013954689937110371d7e5e54","value":" 1.39M/1.39M [00:00&lt;00:00, 5.38MB/s]"}},"f74da5470f84455bb391084713d612c7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e298faf220814043bc6d3b61fca7922c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"dd6641a848324bd9a11b1115a2490358":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"5dd7feecefe4427ea642a09ed44582df":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"768bf865f51b4361ab54a71a7f575a8c":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"fbeddacd9621481ea4f887b2b3cbd1aa":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"557741c013954689937110371d7e5e54":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"c47826b0499346509425e685590d5f43":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_110bde5376f3411297c52ce3c73a90a2","IPY_MODEL_0998620f93a24aef80763a41219a0848","IPY_MODEL_e9c61b74661b48088c5b6cf5c7506181"],"layout":"IPY_MODEL_a37cccce926644a5ac5af901949fc090"}},"110bde5376f3411297c52ce3c73a90a2":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_39d63b3fc8ff4b7bbac1af94a6a36dcd","placeholder":"​","style":"IPY_MODEL_484b703b648d428a9180169662ea8dba","value":"config.json: 100%"}},"0998620f93a24aef80763a41219a0848":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_901e822dae954983af93b239d66ab1f7","max":1208,"min":0,"orientation":"horizontal","style":"IPY_MODEL_232c1224d15d4c3e9a1f2597b33561ae","value":1208}},"e9c61b74661b48088c5b6cf5c7506181":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_58e96833051247188c289ec7371a7f5d","placeholder":"​","style":"IPY_MODEL_dc4c78de96054c628a08ef78db78bd7f","value":" 1.21k/1.21k [00:00&lt;00:00, 101kB/s]"}},"a37cccce926644a5ac5af901949fc090":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"39d63b3fc8ff4b7bbac1af94a6a36dcd":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"484b703b648d428a9180169662ea8dba":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"901e822dae954983af93b239d66ab1f7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"232c1224d15d4c3e9a1f2597b33561ae":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"58e96833051247188c289ec7371a7f5d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"dc4c78de96054c628a08ef78db78bd7f":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"c1755b171d3f46be9ce585af1333c546":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_de9f2944333d497381c50bef14808d86","IPY_MODEL_e6e9f43f1d0748f5b355c7e5d5a10578","IPY_MODEL_bfaf0fe003b24746b02333d1cd47d139"],"layout":"IPY_MODEL_488b028bc93a477aa16bb9b07e41385e"}},"de9f2944333d497381c50bef14808d86":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_4c7eee7634024417a4b50cdbf9f5a64b","placeholder":"​","style":"IPY_MODEL_b98fef4fa9f9494e9181f24cb5df4784","value":"model.safetensors: 100%"}},"e6e9f43f1d0748f5b355c7e5d5a10578":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_43cb54cadd6a47758a552b1a4fa5ff0a","max":891646390,"min":0,"orientation":"horizontal","style":"IPY_MODEL_882717a5c867414b9fc066e93b11013d","value":891646390}},"bfaf0fe003b24746b02333d1cd47d139":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_5f64c418cb0d48399337c6a5b5e70c81","placeholder":"​","style":"IPY_MODEL_fcd35cf7685f4bfd886686c9c34155c3","value":" 892M/892M [00:03&lt;00:00, 244MB/s]"}},"488b028bc93a477aa16bb9b07e41385e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4c7eee7634024417a4b50cdbf9f5a64b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b98fef4fa9f9494e9181f24cb5df4784":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"43cb54cadd6a47758a552b1a4fa5ff0a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"882717a5c867414b9fc066e93b11013d":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"5f64c418cb0d48399337c6a5b5e70c81":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"fcd35cf7685f4bfd886686c9c34155c3":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"ee4bfb3d998642ada1cf9393264ba863":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_d0974701088c43fcb7adceb5bb10a2ff","IPY_MODEL_6fae42746ea04932b36272f6bdfa1d78","IPY_MODEL_bcc54223762940cc83f7f340a027fd60"],"layout":"IPY_MODEL_0281ae709cfb41d1b006f83f2eece52c"}},"d0974701088c43fcb7adceb5bb10a2ff":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_904bad7404f641b3a9919c0c028b053e","placeholder":"​","style":"IPY_MODEL_206521a00dc24c1d90e430056ab0c598","value":"generation_config.json: 100%"}},"6fae42746ea04932b36272f6bdfa1d78":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_67e66826c1c94ad2bffb54f0577d7ecc","max":147,"min":0,"orientation":"horizontal","style":"IPY_MODEL_37d3b6acb4c84516bc82bce3e88798fe","value":147}},"bcc54223762940cc83f7f340a027fd60":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_8c0f398c0ba14e47a93ab394cae3b2a8","placeholder":"​","style":"IPY_MODEL_57ff65722c364925a3e81e62159b2f88","value":" 147/147 [00:00&lt;00:00, 10.3kB/s]"}},"0281ae709cfb41d1b006f83f2eece52c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"904bad7404f641b3a9919c0c028b053e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"206521a00dc24c1d90e430056ab0c598":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"67e66826c1c94ad2bffb54f0577d7ecc":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"37d3b6acb4c84516bc82bce3e88798fe":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"8c0f398c0ba14e47a93ab394cae3b2a8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"57ff65722c364925a3e81e62159b2f88":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"29e51844864f45629fa9499321a0b746":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_ba96f0fbf5c34b9a9bd1817d0a4d61e2","IPY_MODEL_ef37783fa005416ea0cd187f89540d3b","IPY_MODEL_50c1d32b74ea46ca8c4029354384be9b"],"layout":"IPY_MODEL_a50369b9297b4d95a501110b8d07990b"}},"ba96f0fbf5c34b9a9bd1817d0a4d61e2":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_55682ea00f0b4d9eb2b40034d49ff2e4","placeholder":"​","style":"IPY_MODEL_069635092a2244dfa52fb80c1d0a1f74","value":"config.json: 100%"}},"ef37783fa005416ea0cd187f89540d3b":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_5cd0ae4401fb40d494ec9a2611a3849e","max":1206,"min":0,"orientation":"horizontal","style":"IPY_MODEL_c2369811e6d7441895bd185b14f49dd3","value":1206}},"50c1d32b74ea46ca8c4029354384be9b":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a490cd67db0b4b68abe27ddd0ffaafaf","placeholder":"​","style":"IPY_MODEL_41555d54a7d84577b755c2757aa59a27","value":" 1.21k/1.21k [00:00&lt;00:00, 37.0kB/s]"}},"a50369b9297b4d95a501110b8d07990b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"55682ea00f0b4d9eb2b40034d49ff2e4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"069635092a2244dfa52fb80c1d0a1f74":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"5cd0ae4401fb40d494ec9a2611a3849e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c2369811e6d7441895bd185b14f49dd3":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"a490cd67db0b4b68abe27ddd0ffaafaf":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"41555d54a7d84577b755c2757aa59a27":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"42522f92dce0441a9c29c044be467c24":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_3d1a17601b0146acbf771dcdd34fee35","IPY_MODEL_1691878d9fee4d7aa1899e9ca456c92d","IPY_MODEL_c22fa34dadd5420d853a1ee39c2a2c32"],"layout":"IPY_MODEL_3efdcd7617134a06b3517996de18aad0"}},"3d1a17601b0146acbf771dcdd34fee35":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_4499e40f8fde430e96fe9498edd2b7cb","placeholder":"​","style":"IPY_MODEL_05d5a37152484ae79fd61b8fe5f3d02c","value":"model.safetensors: 100%"}},"1691878d9fee4d7aa1899e9ca456c92d":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_a29a708dd0a544e888878effbf5aa0a5","max":242043056,"min":0,"orientation":"horizontal","style":"IPY_MODEL_adb44014c6ed4f34a330857f2e48103f","value":242043056}},"c22fa34dadd5420d853a1ee39c2a2c32":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d32eeffe9d5c495d9caf860f1407fff0","placeholder":"​","style":"IPY_MODEL_d06bb1bfe4ba452cb45e4420386c7d88","value":" 242M/242M [00:01&lt;00:00, 172MB/s]"}},"3efdcd7617134a06b3517996de18aad0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4499e40f8fde430e96fe9498edd2b7cb":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"05d5a37152484ae79fd61b8fe5f3d02c":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"a29a708dd0a544e888878effbf5aa0a5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"adb44014c6ed4f34a330857f2e48103f":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"d32eeffe9d5c495d9caf860f1407fff0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d06bb1bfe4ba452cb45e4420386c7d88":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"9ee5c400f560428292bb4a0d12cf228b":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_72924dbf492c4e59a1833dd05986c550","IPY_MODEL_11ac789836e849c0b6782591ad927e9e","IPY_MODEL_9f9f0f5f850d4862bfdb52a0ca4eeeca"],"layout":"IPY_MODEL_d261687f475a48b8a93ee8ef0787a28c"}},"72924dbf492c4e59a1833dd05986c550":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_dfb9dda51eef4787ad98463592e8da44","placeholder":"​","style":"IPY_MODEL_0a79b770a3814871b53039ff07aa4ea9","value":"generation_config.json: 100%"}},"11ac789836e849c0b6782591ad927e9e":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_70440a4a44ce4ca9a17f065bf46759de","max":147,"min":0,"orientation":"horizontal","style":"IPY_MODEL_c173bbe534e34f2c913303b17e531f51","value":147}},"9f9f0f5f850d4862bfdb52a0ca4eeeca":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_413cba7de78042e0abb8e1db7310a3f1","placeholder":"​","style":"IPY_MODEL_0d26354dbb984e1ab3d8f6a57335b299","value":" 147/147 [00:00&lt;00:00, 5.03kB/s]"}},"d261687f475a48b8a93ee8ef0787a28c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"dfb9dda51eef4787ad98463592e8da44":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0a79b770a3814871b53039ff07aa4ea9":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"70440a4a44ce4ca9a17f065bf46759de":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c173bbe534e34f2c913303b17e531f51":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"413cba7de78042e0abb8e1db7310a3f1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0d26354dbb984e1ab3d8f6a57335b299":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"199c6f7cc2454aa586225fdff35941a2":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_e4cb66087d0d4034ab4fb18a92e20b27","IPY_MODEL_392eec678d4743cd89d3b086d4a26d9b","IPY_MODEL_2af7fe02e41248b79f96a3d14defd792"],"layout":"IPY_MODEL_c7ef8f52378e4b88bc9a7293e0b7d029"}},"e4cb66087d0d4034ab4fb18a92e20b27":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_b066dc3965604a87b9425416610c985a","placeholder":"​","style":"IPY_MODEL_2bf06c70392945d89397e305f34e8941","value":"tokenizer_config.json: 100%"}},"392eec678d4743cd89d3b086d4a26d9b":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_fc848de3566c45e0a11553b1a2802962","max":2324,"min":0,"orientation":"horizontal","style":"IPY_MODEL_b1800d11e79f4b19accd754eb662e1c7","value":2324}},"2af7fe02e41248b79f96a3d14defd792":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d9cd50f271d446949931d12758533f5c","placeholder":"​","style":"IPY_MODEL_17b42d459852498faa24f54805fa388f","value":" 2.32k/2.32k [00:00&lt;00:00, 100kB/s]"}},"c7ef8f52378e4b88bc9a7293e0b7d029":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b066dc3965604a87b9425416610c985a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2bf06c70392945d89397e305f34e8941":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"fc848de3566c45e0a11553b1a2802962":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b1800d11e79f4b19accd754eb662e1c7":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"d9cd50f271d446949931d12758533f5c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"17b42d459852498faa24f54805fa388f":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"8db76b02ef6f41d6952617d35322a9fa":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_f09ba207a15b4ff49bbee44df2625119","IPY_MODEL_441827fd89b5476faad6fe15a2686518","IPY_MODEL_68d09183b2914fe0b6f2bb7455d27367"],"layout":"IPY_MODEL_7b975f07f6f74905833594eacc1f4aac"}},"f09ba207a15b4ff49bbee44df2625119":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_0950891e622e43afa5bb7446f0e03b1d","placeholder":"​","style":"IPY_MODEL_95d9c1e25cb8421cbb24fb28b97a5f20","value":"spiece.model: 100%"}},"441827fd89b5476faad6fe15a2686518":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_e8a3eab41cc047e6bacb4c63d31da863","max":791656,"min":0,"orientation":"horizontal","style":"IPY_MODEL_373be5b835bb4014bd1c6209f47ecdf3","value":791656}},"68d09183b2914fe0b6f2bb7455d27367":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_5747b0afc1cc4b5eaa5d1ea1f4d13930","placeholder":"​","style":"IPY_MODEL_3a3269c0a32f4c9e92e52f6445a854a5","value":" 792k/792k [00:00&lt;00:00, 4.01MB/s]"}},"7b975f07f6f74905833594eacc1f4aac":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0950891e622e43afa5bb7446f0e03b1d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"95d9c1e25cb8421cbb24fb28b97a5f20":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"e8a3eab41cc047e6bacb4c63d31da863":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"373be5b835bb4014bd1c6209f47ecdf3":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"5747b0afc1cc4b5eaa5d1ea1f4d13930":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3a3269c0a32f4c9e92e52f6445a854a5":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"2a37b83c11bf4b95b3bb07defacc48ed":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_26c6f0b90cb34505829af754e3093355","IPY_MODEL_05cab35f67984e5e8910d639e739026c","IPY_MODEL_d204f9dbb73d4d9c8f41daa60ad35b4e"],"layout":"IPY_MODEL_d3eb79b24cff481ea79c70355a7ed2bd"}},"26c6f0b90cb34505829af754e3093355":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d62bb2b12d52406880dd6907d11a317b","placeholder":"​","style":"IPY_MODEL_be18da5b56434324aa006b50f9ae77a1","value":"tokenizer.json: 100%"}},"05cab35f67984e5e8910d639e739026c":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_594c4ee611f64ef3b7f8640fea57f0ed","max":1389353,"min":0,"orientation":"horizontal","style":"IPY_MODEL_d75509aff37e48c6bea9b21b5318effa","value":1389353}},"d204f9dbb73d4d9c8f41daa60ad35b4e":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_1115f99671eb45358c51aa7b9b6d4938","placeholder":"​","style":"IPY_MODEL_561da56b5ee0485eaea2dcaed98b6541","value":" 1.39M/1.39M [00:00&lt;00:00, 7.07MB/s]"}},"d3eb79b24cff481ea79c70355a7ed2bd":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d62bb2b12d52406880dd6907d11a317b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"be18da5b56434324aa006b50f9ae77a1":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"594c4ee611f64ef3b7f8640fea57f0ed":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d75509aff37e48c6bea9b21b5318effa":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"1115f99671eb45358c51aa7b9b6d4938":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"561da56b5ee0485eaea2dcaed98b6541":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"82403c3bdfd944b8820b1840e2c66fd5":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_567468a3a22b45f8b50a1cdbce8438c5","IPY_MODEL_48808ca04ebc4b20b9f9224bf5956c46","IPY_MODEL_6f2fd1b9283c49f1a23b9b12672e723a"],"layout":"IPY_MODEL_daee0456d83049daa8ee85d3aa11c885"}},"567468a3a22b45f8b50a1cdbce8438c5":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_782e21dc4b4e46fab69bd9a4b11ecc2b","placeholder":"​","style":"IPY_MODEL_a6a26f45114e48828d764063faf59fa8","value":"Downloading data: 100%"}},"48808ca04ebc4b20b9f9224bf5956c46":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_7d23e52549434814b97472564a739bf2","max":28914814,"min":0,"orientation":"horizontal","style":"IPY_MODEL_ec8196f735b7410286e0973488b7d563","value":28914814}},"6f2fd1b9283c49f1a23b9b12672e723a":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f84a73def8a64d6687ec262dafc78916","placeholder":"​","style":"IPY_MODEL_1db1a999b2a94f2ab49bf477131cdf98","value":" 28.9M/28.9M [00:00&lt;00:00, 40.3MB/s]"}},"daee0456d83049daa8ee85d3aa11c885":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"782e21dc4b4e46fab69bd9a4b11ecc2b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a6a26f45114e48828d764063faf59fa8":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"7d23e52549434814b97472564a739bf2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ec8196f735b7410286e0973488b7d563":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"f84a73def8a64d6687ec262dafc78916":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1db1a999b2a94f2ab49bf477131cdf98":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"8323b0a6e96b441ca8e3f56648983f39":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_7e816eff335241499fdca241b1574618","IPY_MODEL_98aaf3bef7f4411c88f9111b40318222","IPY_MODEL_9af5ee7c99734de99339b5892bcabeee"],"layout":"IPY_MODEL_180acc32bcf844afa003b0a192ab886b"}},"7e816eff335241499fdca241b1574618":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_4385d5e66db749efa38228696f867aa0","placeholder":"​","style":"IPY_MODEL_fad884d050a243b5a147622eef7d12f4","value":"Downloading data: 100%"}},"98aaf3bef7f4411c88f9111b40318222":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_8946dab706404516bfe41cf0fc5a36b2","max":2639888,"min":0,"orientation":"horizontal","style":"IPY_MODEL_55cea314c38b4db4a608e5669ec22702","value":2639888}},"9af5ee7c99734de99339b5892bcabeee":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_53f9a73cfd714ed886ad2c84dba62ef0","placeholder":"​","style":"IPY_MODEL_573f0c787b1a4164a7e17e2f1a0ed36e","value":" 2.64M/2.64M [00:00&lt;00:00, 20.3MB/s]"}},"180acc32bcf844afa003b0a192ab886b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4385d5e66db749efa38228696f867aa0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"fad884d050a243b5a147622eef7d12f4":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"8946dab706404516bfe41cf0fc5a36b2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"55cea314c38b4db4a608e5669ec22702":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"53f9a73cfd714ed886ad2c84dba62ef0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"573f0c787b1a4164a7e17e2f1a0ed36e":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"6f1443064e46402285fdadb57df768b3":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_e310b9a0a8ef4ab3a3f15a4b376a9477","IPY_MODEL_6fd0f9d0aeb148fd881508c4ea24b4e5","IPY_MODEL_f6637a0ba02848d9bdf43af4b16ead4d"],"layout":"IPY_MODEL_8aa4b66d5a4142efb76e49d2ae87bb49"}},"e310b9a0a8ef4ab3a3f15a4b376a9477":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_0cf72d354d5648d6823cfca1b85c5697","placeholder":"​","style":"IPY_MODEL_945388369ffa48d3ad64422ce061774b","value":"Generating train split: 100%"}},"6fd0f9d0aeb148fd881508c4ea24b4e5":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_1299e4509c2a4e0aa8d9d66c65898c4d","max":45450,"min":0,"orientation":"horizontal","style":"IPY_MODEL_d7d0d24f15034afa8b0a2f4960ce64e4","value":45450}},"f6637a0ba02848d9bdf43af4b16ead4d":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_cc4942dae23149e8a2fb9e22c826f673","placeholder":"​","style":"IPY_MODEL_31e7dfe0af9a4fc681f92008849e0f31","value":" 45450/45450 [00:00&lt;00:00, 121965.73 examples/s]"}},"8aa4b66d5a4142efb76e49d2ae87bb49":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0cf72d354d5648d6823cfca1b85c5697":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"945388369ffa48d3ad64422ce061774b":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"1299e4509c2a4e0aa8d9d66c65898c4d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d7d0d24f15034afa8b0a2f4960ce64e4":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"cc4942dae23149e8a2fb9e22c826f673":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"31e7dfe0af9a4fc681f92008849e0f31":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"387e6aa757e2415daeb073918f36f1ae":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_88286156df064f3cb18df70c0564b140","IPY_MODEL_d63a13bc7bf242189d531f376e52f8c0","IPY_MODEL_09ca0b2e53684e4688bae2b50c4f2450"],"layout":"IPY_MODEL_69d1a488e5da477d95e2242a0f217b61"}},"88286156df064f3cb18df70c0564b140":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_fbbec830c1af40a28a16896d0ab17cb8","placeholder":"​","style":"IPY_MODEL_a71936bb55134b238d7f34397bdc41ba","value":"Generating validation split: 100%"}},"d63a13bc7bf242189d531f376e52f8c0":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_b73c089b65eb4bcd9513b5c8e68159b5","max":2514,"min":0,"orientation":"horizontal","style":"IPY_MODEL_48753c893d9d40e095fa7415a2606354","value":2514}},"09ca0b2e53684e4688bae2b50c4f2450":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a99500f6740a43008999530b6bc9dfdf","placeholder":"​","style":"IPY_MODEL_d936c910bc3544068bbbda821404a6a3","value":" 2514/2514 [00:00&lt;00:00, 17622.76 examples/s]"}},"69d1a488e5da477d95e2242a0f217b61":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"fbbec830c1af40a28a16896d0ab17cb8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a71936bb55134b238d7f34397bdc41ba":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"b73c089b65eb4bcd9513b5c8e68159b5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"48753c893d9d40e095fa7415a2606354":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"a99500f6740a43008999530b6bc9dfdf":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d936c910bc3544068bbbda821404a6a3":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30699,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"'''\n    The following script was written and run on Kaggle's P100 GPU (faster than Goggle colab's T4 GPU).\n'''","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Loading the model and the tokenizer ....","metadata":{"id":"EagWER85I4JN"}},{"cell_type":"code","source":"import torch\nfrom transformers import T5Tokenizer, T5ForConditionalGeneration\nimport warnings\nwarnings.simplefilter(action='ignore')","metadata":{"id":"2wCPYGMjgpTx"},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"model_name = 't5-base'\ntokenizer = T5Tokenizer.from_pretrained(model_name)\nmodel = T5ForConditionalGeneration.from_pretrained(model_name)\nmodel","metadata":{"id":"bAOZbCBPg_vv","colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["5f7f0ccfc8d7413192d0bfe16a345ba2","b4828e7e2ab545ae9617f8d73dfa7117","bd242b09a77b46f2bc671f164da6e27e","8dbab1308ba048f7bfd1112709433350","98ae9f32c1ff479abe7090f3e5627514","2fa307bd7abf4797bc97c37660764c63","db93551c4a4245c2a00fb5e748295e4b","7c2f2245c5794a8382075c821fbe14c9","e5d8bfb8b1224642a169176d829e7c01","0e5801064ded473080e793bccc1d6ce2","2241b588baf041f081ccf21e955cce3b","c71c0cf80c924c209c969598dfd5272e","8d56c9cfa8044d2d8ca8195db76e22d7","d769cdbd8da5401d9c08bd2bb1ddfd0d","ecb77b7e9adb40a7a197cc5c955192a8","f74da5470f84455bb391084713d612c7","e298faf220814043bc6d3b61fca7922c","dd6641a848324bd9a11b1115a2490358","5dd7feecefe4427ea642a09ed44582df","768bf865f51b4361ab54a71a7f575a8c","fbeddacd9621481ea4f887b2b3cbd1aa","557741c013954689937110371d7e5e54","c47826b0499346509425e685590d5f43","110bde5376f3411297c52ce3c73a90a2","0998620f93a24aef80763a41219a0848","e9c61b74661b48088c5b6cf5c7506181","a37cccce926644a5ac5af901949fc090","39d63b3fc8ff4b7bbac1af94a6a36dcd","484b703b648d428a9180169662ea8dba","901e822dae954983af93b239d66ab1f7","232c1224d15d4c3e9a1f2597b33561ae","58e96833051247188c289ec7371a7f5d","dc4c78de96054c628a08ef78db78bd7f","c1755b171d3f46be9ce585af1333c546","de9f2944333d497381c50bef14808d86","e6e9f43f1d0748f5b355c7e5d5a10578","bfaf0fe003b24746b02333d1cd47d139","488b028bc93a477aa16bb9b07e41385e","4c7eee7634024417a4b50cdbf9f5a64b","b98fef4fa9f9494e9181f24cb5df4784","43cb54cadd6a47758a552b1a4fa5ff0a","882717a5c867414b9fc066e93b11013d","5f64c418cb0d48399337c6a5b5e70c81","fcd35cf7685f4bfd886686c9c34155c3","ee4bfb3d998642ada1cf9393264ba863","d0974701088c43fcb7adceb5bb10a2ff","6fae42746ea04932b36272f6bdfa1d78","bcc54223762940cc83f7f340a027fd60","0281ae709cfb41d1b006f83f2eece52c","904bad7404f641b3a9919c0c028b053e","206521a00dc24c1d90e430056ab0c598","67e66826c1c94ad2bffb54f0577d7ecc","37d3b6acb4c84516bc82bce3e88798fe","8c0f398c0ba14e47a93ab394cae3b2a8","57ff65722c364925a3e81e62159b2f88"]},"outputId":"e56f6590-1564-431b-8a41-f923ca498763"},"execution_count":2,"outputs":[{"output_type":"display_data","data":{"text/plain":["spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5f7f0ccfc8d7413192d0bfe16a345ba2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["tokenizer.json:   0%|          | 0.00/1.39M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c71c0cf80c924c209c969598dfd5272e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["config.json:   0%|          | 0.00/1.21k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c47826b0499346509425e685590d5f43"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":"You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n\nSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"},{"output_type":"display_data","data":{"text/plain":["model.safetensors:   0%|          | 0.00/892M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c1755b171d3f46be9ce585af1333c546"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["generation_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ee4bfb3d998642ada1cf9393264ba863"}},"metadata":{}},{"output_type":"execute_result","execution_count":2,"data":{"text/plain":["T5ForConditionalGeneration(\n","  (shared): Embedding(32128, 768)\n","  (encoder): T5Stack(\n","    (embed_tokens): Embedding(32128, 768)\n","    (block): ModuleList(\n","      (0): T5Block(\n","        (layer): ModuleList(\n","          (0): T5LayerSelfAttention(\n","            (SelfAttention): T5Attention(\n","              (q): Linear(in_features=768, out_features=768, bias=False)\n","              (k): Linear(in_features=768, out_features=768, bias=False)\n","              (v): Linear(in_features=768, out_features=768, bias=False)\n","              (o): Linear(in_features=768, out_features=768, bias=False)\n","              (relative_attention_bias): Embedding(32, 12)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): T5LayerFF(\n","            (DenseReluDense): T5DenseActDense(\n","              (wi): Linear(in_features=768, out_features=3072, bias=False)\n","              (wo): Linear(in_features=3072, out_features=768, bias=False)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","              (act): ReLU()\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","      (1-11): 11 x T5Block(\n","        (layer): ModuleList(\n","          (0): T5LayerSelfAttention(\n","            (SelfAttention): T5Attention(\n","              (q): Linear(in_features=768, out_features=768, bias=False)\n","              (k): Linear(in_features=768, out_features=768, bias=False)\n","              (v): Linear(in_features=768, out_features=768, bias=False)\n","              (o): Linear(in_features=768, out_features=768, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): T5LayerFF(\n","            (DenseReluDense): T5DenseActDense(\n","              (wi): Linear(in_features=768, out_features=3072, bias=False)\n","              (wo): Linear(in_features=3072, out_features=768, bias=False)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","              (act): ReLU()\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","    )\n","    (final_layer_norm): T5LayerNorm()\n","    (dropout): Dropout(p=0.1, inplace=False)\n","  )\n","  (decoder): T5Stack(\n","    (embed_tokens): Embedding(32128, 768)\n","    (block): ModuleList(\n","      (0): T5Block(\n","        (layer): ModuleList(\n","          (0): T5LayerSelfAttention(\n","            (SelfAttention): T5Attention(\n","              (q): Linear(in_features=768, out_features=768, bias=False)\n","              (k): Linear(in_features=768, out_features=768, bias=False)\n","              (v): Linear(in_features=768, out_features=768, bias=False)\n","              (o): Linear(in_features=768, out_features=768, bias=False)\n","              (relative_attention_bias): Embedding(32, 12)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): T5LayerCrossAttention(\n","            (EncDecAttention): T5Attention(\n","              (q): Linear(in_features=768, out_features=768, bias=False)\n","              (k): Linear(in_features=768, out_features=768, bias=False)\n","              (v): Linear(in_features=768, out_features=768, bias=False)\n","              (o): Linear(in_features=768, out_features=768, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (2): T5LayerFF(\n","            (DenseReluDense): T5DenseActDense(\n","              (wi): Linear(in_features=768, out_features=3072, bias=False)\n","              (wo): Linear(in_features=3072, out_features=768, bias=False)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","              (act): ReLU()\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","      (1-11): 11 x T5Block(\n","        (layer): ModuleList(\n","          (0): T5LayerSelfAttention(\n","            (SelfAttention): T5Attention(\n","              (q): Linear(in_features=768, out_features=768, bias=False)\n","              (k): Linear(in_features=768, out_features=768, bias=False)\n","              (v): Linear(in_features=768, out_features=768, bias=False)\n","              (o): Linear(in_features=768, out_features=768, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): T5LayerCrossAttention(\n","            (EncDecAttention): T5Attention(\n","              (q): Linear(in_features=768, out_features=768, bias=False)\n","              (k): Linear(in_features=768, out_features=768, bias=False)\n","              (v): Linear(in_features=768, out_features=768, bias=False)\n","              (o): Linear(in_features=768, out_features=768, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (2): T5LayerFF(\n","            (DenseReluDense): T5DenseActDense(\n","              (wi): Linear(in_features=768, out_features=3072, bias=False)\n","              (wo): Linear(in_features=3072, out_features=768, bias=False)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","              (act): ReLU()\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","    )\n","    (final_layer_norm): T5LayerNorm()\n","    (dropout): Dropout(p=0.1, inplace=False)\n","  )\n","  (lm_head): Linear(in_features=768, out_features=32128, bias=False)\n",")"]},"metadata":{}}]},{"cell_type":"markdown","source":"Verifying Summarization ..","metadata":{"id":"nkZf6rWVg6t5"}},{"cell_type":"code","source":"'''\n  The following is an introductory paragraph from a Wikipedia page on Laplace Distribution\n'''\npara = \"In probability theory and statistics, the Laplace distribution is a continuous probability distribution named after Pierre-Simon Laplace. It is also sometimes called the double exponential distribution, because it can be thought of as two exponential distributions (with an additional location parameter) spliced together along the abscissa, although the term is also sometimes used to refer to the Gumbel distribution. The difference between two independent identically distributed exponential random variables is governed by a Laplace distribution, as is a Brownian motion evaluated at an exponentially distributed random time[citation needed]. Increments of Laplace motion or a variance gamma process evaluated over the time scale also have a Laplace distribution.\"","metadata":{"id":"BhRCcyfj5Xjs"},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"inputs=tokenizer.encode(\"summarize: \" + para,return_tensors='pt', max_length=512, truncation=True)","metadata":{"id":"eJ_Ur2I75X_d"},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"output = model.generate(inputs, max_length=50)","metadata":{"id":"P2pJ5RLW5Yl4"},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"summary=tokenizer.decode(output[0], skip_special_tokens=True)\nprint(summary)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"R2L7iprM5Y-E","outputId":"9af14ce6-5c73-4b66-a644-897a7c2262e5"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":"the Laplace distribution is a continuous probability distribution named after Pierre-Simon Laplace. it can be thought of as two exponential distributions spliced together along the abscissa.\n"}]},{"cell_type":"markdown","source":"Verifying QnA task ...","metadata":{"id":"nq-eS9SO57I2"}},{"cell_type":"code","source":"ques = \"What is the capital of Spain?\"\ninputs=tokenizer.encode(\"question: \" + ques,return_tensors='pt', max_length=512, truncation=True)","metadata":{"id":"ANMOU96f52u0"},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"out = model.generate(inputs, max_length=5, num_beams=2, early_stopping=True) # can use early_stopping for one-word answer\nprint(tokenizer.decode(out[0], skip_special_tokens=True))","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rLLLjcLKAiKa","outputId":"2c89cf13-7a1b-4ca6-fc68-b848037358b8"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":"Madrid\n"}]},{"cell_type":"markdown","source":"Verifying Translation ...","metadata":{"id":"QSu4ujOqDcGa"}},{"cell_type":"code","source":"sent = \"Hi, My name is Aditya. Nice to meet me ;)\"\ninputs=tokenizer.encode(\"translate English to French: \" + sent,return_tensors='pt', max_length=512, truncation=True)","metadata":{"id":"SeRe0eltDeVj"},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"out = model.generate(inputs, max_length=50)\nprint(tokenizer.decode(out[0], skip_special_tokens=True))","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OzqhtNkFDuf-","outputId":"4a7c55c0-731c-4500-8336-44ec6f6b8412"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":"Bonjour, Je m'appelle Aditya, j'ai eu la chance de me rencontrer ;)\n"}]},{"cell_type":"markdown","source":"Deep diving into Model Params ...","metadata":{"id":"pLWWWQ40FGKV"}},{"cell_type":"code","source":"# Printing the names and dimensions of all the layers as well as total number of params in the model ...\ntot_params=0\nfor n,p in model.state_dict().items():\n  print(\"Layer Name: \"+n, \"; Layer Shape: \"+str(p.shape))\n  tot_params+=p.numel()\nprint(\"--\"*70)\nprint(f\"Total number of parameters in {model_name}: \"+str(tot_params)) # Since we've included the params for LM_Head as well, the # of params must be close to 300M for t5-base, as expected!","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mDNpcGDeDw53","outputId":"eba6e43b-3dd8-4924-c502-03724d72507c"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":"Layer Name: shared.weight ; Layer Shape: torch.Size([32128, 768])\n\nLayer Name: encoder.embed_tokens.weight ; Layer Shape: torch.Size([32128, 768])\n\nLayer Name: encoder.block.0.layer.0.SelfAttention.q.weight ; Layer Shape: torch.Size([768, 768])\n\nLayer Name: encoder.block.0.layer.0.SelfAttention.k.weight ; Layer Shape: torch.Size([768, 768])\n\nLayer Name: encoder.block.0.layer.0.SelfAttention.v.weight ; Layer Shape: torch.Size([768, 768])\n\nLayer Name: encoder.block.0.layer.0.SelfAttention.o.weight ; Layer Shape: torch.Size([768, 768])\n\nLayer Name: encoder.block.0.layer.0.SelfAttention.relative_attention_bias.weight ; Layer Shape: torch.Size([32, 12])\n\nLayer Name: encoder.block.0.layer.0.layer_norm.weight ; Layer Shape: torch.Size([768])\n\nLayer Name: encoder.block.0.layer.1.DenseReluDense.wi.weight ; Layer Shape: torch.Size([3072, 768])\n\nLayer Name: encoder.block.0.layer.1.DenseReluDense.wo.weight ; Layer Shape: torch.Size([768, 3072])\n\nLayer Name: encoder.block.0.layer.1.layer_norm.weight ; Layer Shape: torch.Size([768])\n\nLayer Name: encoder.block.1.layer.0.SelfAttention.q.weight ; Layer Shape: torch.Size([768, 768])\n\nLayer Name: encoder.block.1.layer.0.SelfAttention.k.weight ; Layer Shape: torch.Size([768, 768])\n\nLayer Name: encoder.block.1.layer.0.SelfAttention.v.weight ; Layer Shape: torch.Size([768, 768])\n\nLayer Name: encoder.block.1.layer.0.SelfAttention.o.weight ; Layer Shape: torch.Size([768, 768])\n\nLayer Name: encoder.block.1.layer.0.layer_norm.weight ; Layer Shape: torch.Size([768])\n\nLayer Name: encoder.block.1.layer.1.DenseReluDense.wi.weight ; Layer Shape: torch.Size([3072, 768])\n\nLayer Name: encoder.block.1.layer.1.DenseReluDense.wo.weight ; Layer Shape: torch.Size([768, 3072])\n\nLayer Name: encoder.block.1.layer.1.layer_norm.weight ; Layer Shape: torch.Size([768])\n\nLayer Name: encoder.block.2.layer.0.SelfAttention.q.weight ; Layer Shape: torch.Size([768, 768])\n\nLayer Name: encoder.block.2.layer.0.SelfAttention.k.weight ; Layer Shape: torch.Size([768, 768])\n\nLayer Name: encoder.block.2.layer.0.SelfAttention.v.weight ; Layer Shape: torch.Size([768, 768])\n\nLayer Name: encoder.block.2.layer.0.SelfAttention.o.weight ; Layer Shape: torch.Size([768, 768])\n\nLayer Name: encoder.block.2.layer.0.layer_norm.weight ; Layer Shape: torch.Size([768])\n\nLayer Name: encoder.block.2.layer.1.DenseReluDense.wi.weight ; Layer Shape: torch.Size([3072, 768])\n\nLayer Name: encoder.block.2.layer.1.DenseReluDense.wo.weight ; Layer Shape: torch.Size([768, 3072])\n\nLayer Name: encoder.block.2.layer.1.layer_norm.weight ; Layer Shape: torch.Size([768])\n\nLayer Name: encoder.block.3.layer.0.SelfAttention.q.weight ; Layer Shape: torch.Size([768, 768])\n\nLayer Name: encoder.block.3.layer.0.SelfAttention.k.weight ; Layer Shape: torch.Size([768, 768])\n\nLayer Name: encoder.block.3.layer.0.SelfAttention.v.weight ; Layer Shape: torch.Size([768, 768])\n\nLayer Name: encoder.block.3.layer.0.SelfAttention.o.weight ; Layer Shape: torch.Size([768, 768])\n\nLayer Name: encoder.block.3.layer.0.layer_norm.weight ; Layer Shape: torch.Size([768])\n\nLayer Name: encoder.block.3.layer.1.DenseReluDense.wi.weight ; Layer Shape: torch.Size([3072, 768])\n\nLayer Name: encoder.block.3.layer.1.DenseReluDense.wo.weight ; Layer Shape: torch.Size([768, 3072])\n\nLayer Name: encoder.block.3.layer.1.layer_norm.weight ; Layer Shape: torch.Size([768])\n\nLayer Name: encoder.block.4.layer.0.SelfAttention.q.weight ; Layer Shape: torch.Size([768, 768])\n\nLayer Name: encoder.block.4.layer.0.SelfAttention.k.weight ; Layer Shape: torch.Size([768, 768])\n\nLayer Name: encoder.block.4.layer.0.SelfAttention.v.weight ; Layer Shape: torch.Size([768, 768])\n\nLayer Name: encoder.block.4.layer.0.SelfAttention.o.weight ; Layer Shape: torch.Size([768, 768])\n\nLayer Name: encoder.block.4.layer.0.layer_norm.weight ; Layer Shape: torch.Size([768])\n\nLayer Name: encoder.block.4.layer.1.DenseReluDense.wi.weight ; Layer Shape: torch.Size([3072, 768])\n\nLayer Name: encoder.block.4.layer.1.DenseReluDense.wo.weight ; Layer Shape: torch.Size([768, 3072])\n\nLayer Name: encoder.block.4.layer.1.layer_norm.weight ; Layer Shape: torch.Size([768])\n\nLayer Name: encoder.block.5.layer.0.SelfAttention.q.weight ; Layer Shape: torch.Size([768, 768])\n\nLayer Name: encoder.block.5.layer.0.SelfAttention.k.weight ; Layer Shape: torch.Size([768, 768])\n\nLayer Name: encoder.block.5.layer.0.SelfAttention.v.weight ; Layer Shape: torch.Size([768, 768])\n\nLayer Name: encoder.block.5.layer.0.SelfAttention.o.weight ; Layer Shape: torch.Size([768, 768])\n\nLayer Name: encoder.block.5.layer.0.layer_norm.weight ; Layer Shape: torch.Size([768])\n\nLayer Name: encoder.block.5.layer.1.DenseReluDense.wi.weight ; Layer Shape: torch.Size([3072, 768])\n\nLayer Name: encoder.block.5.layer.1.DenseReluDense.wo.weight ; Layer Shape: torch.Size([768, 3072])\n\nLayer Name: encoder.block.5.layer.1.layer_norm.weight ; Layer Shape: torch.Size([768])\n\nLayer Name: encoder.block.6.layer.0.SelfAttention.q.weight ; Layer Shape: torch.Size([768, 768])\n\nLayer Name: encoder.block.6.layer.0.SelfAttention.k.weight ; Layer Shape: torch.Size([768, 768])\n\nLayer Name: encoder.block.6.layer.0.SelfAttention.v.weight ; Layer Shape: torch.Size([768, 768])\n\nLayer Name: encoder.block.6.layer.0.SelfAttention.o.weight ; Layer Shape: torch.Size([768, 768])\n\nLayer Name: encoder.block.6.layer.0.layer_norm.weight ; Layer Shape: torch.Size([768])\n\nLayer Name: encoder.block.6.layer.1.DenseReluDense.wi.weight ; Layer Shape: torch.Size([3072, 768])\n\nLayer Name: encoder.block.6.layer.1.DenseReluDense.wo.weight ; Layer Shape: torch.Size([768, 3072])\n\nLayer Name: encoder.block.6.layer.1.layer_norm.weight ; Layer Shape: torch.Size([768])\n\nLayer Name: encoder.block.7.layer.0.SelfAttention.q.weight ; Layer Shape: torch.Size([768, 768])\n\nLayer Name: encoder.block.7.layer.0.SelfAttention.k.weight ; Layer Shape: torch.Size([768, 768])\n\nLayer Name: encoder.block.7.layer.0.SelfAttention.v.weight ; Layer Shape: torch.Size([768, 768])\n\nLayer Name: encoder.block.7.layer.0.SelfAttention.o.weight ; Layer Shape: torch.Size([768, 768])\n\nLayer Name: encoder.block.7.layer.0.layer_norm.weight ; Layer Shape: torch.Size([768])\n\nLayer Name: encoder.block.7.layer.1.DenseReluDense.wi.weight ; Layer Shape: torch.Size([3072, 768])\n\nLayer Name: encoder.block.7.layer.1.DenseReluDense.wo.weight ; Layer Shape: torch.Size([768, 3072])\n\nLayer Name: encoder.block.7.layer.1.layer_norm.weight ; Layer Shape: torch.Size([768])\n\nLayer Name: encoder.block.8.layer.0.SelfAttention.q.weight ; Layer Shape: torch.Size([768, 768])\n\nLayer Name: encoder.block.8.layer.0.SelfAttention.k.weight ; Layer Shape: torch.Size([768, 768])\n\nLayer Name: encoder.block.8.layer.0.SelfAttention.v.weight ; Layer Shape: torch.Size([768, 768])\n\nLayer Name: encoder.block.8.layer.0.SelfAttention.o.weight ; Layer Shape: torch.Size([768, 768])\n\nLayer Name: encoder.block.8.layer.0.layer_norm.weight ; Layer Shape: torch.Size([768])\n\nLayer Name: encoder.block.8.layer.1.DenseReluDense.wi.weight ; Layer Shape: torch.Size([3072, 768])\n\nLayer Name: encoder.block.8.layer.1.DenseReluDense.wo.weight ; Layer Shape: torch.Size([768, 3072])\n\nLayer Name: encoder.block.8.layer.1.layer_norm.weight ; Layer Shape: torch.Size([768])\n\nLayer Name: encoder.block.9.layer.0.SelfAttention.q.weight ; Layer Shape: torch.Size([768, 768])\n\nLayer Name: encoder.block.9.layer.0.SelfAttention.k.weight ; Layer Shape: torch.Size([768, 768])\n\nLayer Name: encoder.block.9.layer.0.SelfAttention.v.weight ; Layer Shape: torch.Size([768, 768])\n\nLayer Name: encoder.block.9.layer.0.SelfAttention.o.weight ; Layer Shape: torch.Size([768, 768])\n\nLayer Name: encoder.block.9.layer.0.layer_norm.weight ; Layer Shape: torch.Size([768])\n\nLayer Name: encoder.block.9.layer.1.DenseReluDense.wi.weight ; Layer Shape: torch.Size([3072, 768])\n\nLayer Name: encoder.block.9.layer.1.DenseReluDense.wo.weight ; Layer Shape: torch.Size([768, 3072])\n\nLayer Name: encoder.block.9.layer.1.layer_norm.weight ; Layer Shape: torch.Size([768])\n\nLayer Name: encoder.block.10.layer.0.SelfAttention.q.weight ; Layer Shape: torch.Size([768, 768])\n\nLayer Name: encoder.block.10.layer.0.SelfAttention.k.weight ; Layer Shape: torch.Size([768, 768])\n\nLayer Name: encoder.block.10.layer.0.SelfAttention.v.weight ; Layer Shape: torch.Size([768, 768])\n\nLayer Name: encoder.block.10.layer.0.SelfAttention.o.weight ; Layer Shape: torch.Size([768, 768])\n\nLayer Name: encoder.block.10.layer.0.layer_norm.weight ; Layer Shape: torch.Size([768])\n\nLayer Name: encoder.block.10.layer.1.DenseReluDense.wi.weight ; Layer Shape: torch.Size([3072, 768])\n\nLayer Name: encoder.block.10.layer.1.DenseReluDense.wo.weight ; Layer Shape: torch.Size([768, 3072])\n\nLayer Name: encoder.block.10.layer.1.layer_norm.weight ; Layer Shape: torch.Size([768])\n\nLayer Name: encoder.block.11.layer.0.SelfAttention.q.weight ; Layer Shape: torch.Size([768, 768])\n\nLayer Name: encoder.block.11.layer.0.SelfAttention.k.weight ; Layer Shape: torch.Size([768, 768])\n\nLayer Name: encoder.block.11.layer.0.SelfAttention.v.weight ; Layer Shape: torch.Size([768, 768])\n\nLayer Name: encoder.block.11.layer.0.SelfAttention.o.weight ; Layer Shape: torch.Size([768, 768])\n\nLayer Name: encoder.block.11.layer.0.layer_norm.weight ; Layer Shape: torch.Size([768])\n\nLayer Name: encoder.block.11.layer.1.DenseReluDense.wi.weight ; Layer Shape: torch.Size([3072, 768])\n\nLayer Name: encoder.block.11.layer.1.DenseReluDense.wo.weight ; Layer Shape: torch.Size([768, 3072])\n\nLayer Name: encoder.block.11.layer.1.layer_norm.weight ; Layer Shape: torch.Size([768])\n\nLayer Name: encoder.final_layer_norm.weight ; Layer Shape: torch.Size([768])\n\nLayer Name: decoder.embed_tokens.weight ; Layer Shape: torch.Size([32128, 768])\n\nLayer Name: decoder.block.0.layer.0.SelfAttention.q.weight ; Layer Shape: torch.Size([768, 768])\n\nLayer Name: decoder.block.0.layer.0.SelfAttention.k.weight ; Layer Shape: torch.Size([768, 768])\n\nLayer Name: decoder.block.0.layer.0.SelfAttention.v.weight ; Layer Shape: torch.Size([768, 768])\n\nLayer Name: decoder.block.0.layer.0.SelfAttention.o.weight ; Layer Shape: torch.Size([768, 768])\n\nLayer Name: decoder.block.0.layer.0.SelfAttention.relative_attention_bias.weight ; Layer Shape: torch.Size([32, 12])\n\nLayer Name: decoder.block.0.layer.0.layer_norm.weight ; Layer Shape: torch.Size([768])\n\nLayer Name: decoder.block.0.layer.1.EncDecAttention.q.weight ; Layer Shape: torch.Size([768, 768])\n\nLayer Name: decoder.block.0.layer.1.EncDecAttention.k.weight ; Layer Shape: torch.Size([768, 768])\n\nLayer Name: decoder.block.0.layer.1.EncDecAttention.v.weight ; Layer Shape: torch.Size([768, 768])\n\nLayer Name: decoder.block.0.layer.1.EncDecAttention.o.weight ; Layer Shape: torch.Size([768, 768])\n\nLayer Name: decoder.block.0.layer.1.layer_norm.weight ; Layer Shape: torch.Size([768])\n\nLayer Name: decoder.block.0.layer.2.DenseReluDense.wi.weight ; Layer Shape: torch.Size([3072, 768])\n\nLayer Name: decoder.block.0.layer.2.DenseReluDense.wo.weight ; Layer Shape: torch.Size([768, 3072])\n\nLayer Name: decoder.block.0.layer.2.layer_norm.weight ; Layer Shape: torch.Size([768])\n\nLayer Name: decoder.block.1.layer.0.SelfAttention.q.weight ; Layer Shape: torch.Size([768, 768])\n\nLayer Name: decoder.block.1.layer.0.SelfAttention.k.weight ; Layer Shape: torch.Size([768, 768])\n\nLayer Name: decoder.block.1.layer.0.SelfAttention.v.weight ; Layer Shape: torch.Size([768, 768])\n\nLayer Name: decoder.block.1.layer.0.SelfAttention.o.weight ; Layer Shape: torch.Size([768, 768])\n\nLayer Name: decoder.block.1.layer.0.layer_norm.weight ; Layer Shape: torch.Size([768])\n\nLayer Name: decoder.block.1.layer.1.EncDecAttention.q.weight ; Layer Shape: torch.Size([768, 768])\n\nLayer Name: decoder.block.1.layer.1.EncDecAttention.k.weight ; Layer Shape: torch.Size([768, 768])\n\nLayer Name: decoder.block.1.layer.1.EncDecAttention.v.weight ; Layer Shape: torch.Size([768, 768])\n\nLayer Name: decoder.block.1.layer.1.EncDecAttention.o.weight ; Layer Shape: torch.Size([768, 768])\n\nLayer Name: decoder.block.1.layer.1.layer_norm.weight ; Layer Shape: torch.Size([768])\n\nLayer Name: decoder.block.1.layer.2.DenseReluDense.wi.weight ; Layer Shape: torch.Size([3072, 768])\n\nLayer Name: decoder.block.1.layer.2.DenseReluDense.wo.weight ; Layer Shape: torch.Size([768, 3072])\n\nLayer Name: decoder.block.1.layer.2.layer_norm.weight ; Layer Shape: torch.Size([768])\n\nLayer Name: decoder.block.2.layer.0.SelfAttention.q.weight ; Layer Shape: torch.Size([768, 768])\n\nLayer Name: decoder.block.2.layer.0.SelfAttention.k.weight ; Layer Shape: torch.Size([768, 768])\n\nLayer Name: decoder.block.2.layer.0.SelfAttention.v.weight ; Layer Shape: torch.Size([768, 768])\n\nLayer Name: decoder.block.2.layer.0.SelfAttention.o.weight ; Layer Shape: torch.Size([768, 768])\n\nLayer Name: decoder.block.2.layer.0.layer_norm.weight ; Layer Shape: torch.Size([768])\n\nLayer Name: decoder.block.2.layer.1.EncDecAttention.q.weight ; Layer Shape: torch.Size([768, 768])\n\nLayer Name: decoder.block.2.layer.1.EncDecAttention.k.weight ; Layer Shape: torch.Size([768, 768])\n\nLayer Name: decoder.block.2.layer.1.EncDecAttention.v.weight ; Layer Shape: torch.Size([768, 768])\n\nLayer Name: decoder.block.2.layer.1.EncDecAttention.o.weight ; Layer Shape: torch.Size([768, 768])\n\nLayer Name: decoder.block.2.layer.1.layer_norm.weight ; Layer Shape: torch.Size([768])\n\nLayer Name: decoder.block.2.layer.2.DenseReluDense.wi.weight ; Layer Shape: torch.Size([3072, 768])\n\nLayer Name: decoder.block.2.layer.2.DenseReluDense.wo.weight ; Layer Shape: torch.Size([768, 3072])\n\nLayer Name: decoder.block.2.layer.2.layer_norm.weight ; Layer Shape: torch.Size([768])\n\nLayer Name: decoder.block.3.layer.0.SelfAttention.q.weight ; Layer Shape: torch.Size([768, 768])\n\nLayer Name: decoder.block.3.layer.0.SelfAttention.k.weight ; Layer Shape: torch.Size([768, 768])\n\nLayer Name: decoder.block.3.layer.0.SelfAttention.v.weight ; Layer Shape: torch.Size([768, 768])\n\nLayer Name: decoder.block.3.layer.0.SelfAttention.o.weight ; Layer Shape: torch.Size([768, 768])\n\nLayer Name: decoder.block.3.layer.0.layer_norm.weight ; Layer Shape: torch.Size([768])\n\nLayer Name: decoder.block.3.layer.1.EncDecAttention.q.weight ; Layer Shape: torch.Size([768, 768])\n\nLayer Name: decoder.block.3.layer.1.EncDecAttention.k.weight ; Layer Shape: torch.Size([768, 768])\n\nLayer Name: decoder.block.3.layer.1.EncDecAttention.v.weight ; Layer Shape: torch.Size([768, 768])\n\nLayer Name: decoder.block.3.layer.1.EncDecAttention.o.weight ; Layer Shape: torch.Size([768, 768])\n\nLayer Name: decoder.block.3.layer.1.layer_norm.weight ; Layer Shape: torch.Size([768])\n\nLayer Name: decoder.block.3.layer.2.DenseReluDense.wi.weight ; Layer Shape: torch.Size([3072, 768])\n\nLayer Name: decoder.block.3.layer.2.DenseReluDense.wo.weight ; Layer Shape: torch.Size([768, 3072])\n\nLayer Name: decoder.block.3.layer.2.layer_norm.weight ; Layer Shape: torch.Size([768])\n\nLayer Name: decoder.block.4.layer.0.SelfAttention.q.weight ; Layer Shape: torch.Size([768, 768])\n\nLayer Name: decoder.block.4.layer.0.SelfAttention.k.weight ; Layer Shape: torch.Size([768, 768])\n\nLayer Name: decoder.block.4.layer.0.SelfAttention.v.weight ; Layer Shape: torch.Size([768, 768])\n\nLayer Name: decoder.block.4.layer.0.SelfAttention.o.weight ; Layer Shape: torch.Size([768, 768])\n\nLayer Name: decoder.block.4.layer.0.layer_norm.weight ; Layer Shape: torch.Size([768])\n\nLayer Name: decoder.block.4.layer.1.EncDecAttention.q.weight ; Layer Shape: torch.Size([768, 768])\n\nLayer Name: decoder.block.4.layer.1.EncDecAttention.k.weight ; Layer Shape: torch.Size([768, 768])\n\nLayer Name: decoder.block.4.layer.1.EncDecAttention.v.weight ; Layer Shape: torch.Size([768, 768])\n\nLayer Name: decoder.block.4.layer.1.EncDecAttention.o.weight ; Layer Shape: torch.Size([768, 768])\n\nLayer Name: decoder.block.4.layer.1.layer_norm.weight ; Layer Shape: torch.Size([768])\n\nLayer Name: decoder.block.4.layer.2.DenseReluDense.wi.weight ; Layer Shape: torch.Size([3072, 768])\n\nLayer Name: decoder.block.4.layer.2.DenseReluDense.wo.weight ; Layer Shape: torch.Size([768, 3072])\n\nLayer Name: decoder.block.4.layer.2.layer_norm.weight ; Layer Shape: torch.Size([768])\n\nLayer Name: decoder.block.5.layer.0.SelfAttention.q.weight ; Layer Shape: torch.Size([768, 768])\n\nLayer Name: decoder.block.5.layer.0.SelfAttention.k.weight ; Layer Shape: torch.Size([768, 768])\n\nLayer Name: decoder.block.5.layer.0.SelfAttention.v.weight ; Layer Shape: torch.Size([768, 768])\n\nLayer Name: decoder.block.5.layer.0.SelfAttention.o.weight ; Layer Shape: torch.Size([768, 768])\n\nLayer Name: decoder.block.5.layer.0.layer_norm.weight ; Layer Shape: torch.Size([768])\n\nLayer Name: decoder.block.5.layer.1.EncDecAttention.q.weight ; Layer Shape: torch.Size([768, 768])\n\nLayer Name: decoder.block.5.layer.1.EncDecAttention.k.weight ; Layer Shape: torch.Size([768, 768])\n\nLayer Name: decoder.block.5.layer.1.EncDecAttention.v.weight ; Layer Shape: torch.Size([768, 768])\n\nLayer Name: decoder.block.5.layer.1.EncDecAttention.o.weight ; Layer Shape: torch.Size([768, 768])\n\nLayer Name: decoder.block.5.layer.1.layer_norm.weight ; Layer Shape: torch.Size([768])\n\nLayer Name: decoder.block.5.layer.2.DenseReluDense.wi.weight ; Layer Shape: torch.Size([3072, 768])\n\nLayer Name: decoder.block.5.layer.2.DenseReluDense.wo.weight ; Layer Shape: torch.Size([768, 3072])\n\nLayer Name: decoder.block.5.layer.2.layer_norm.weight ; Layer Shape: torch.Size([768])\n\nLayer Name: decoder.block.6.layer.0.SelfAttention.q.weight ; Layer Shape: torch.Size([768, 768])\n\nLayer Name: decoder.block.6.layer.0.SelfAttention.k.weight ; Layer Shape: torch.Size([768, 768])\n\nLayer Name: decoder.block.6.layer.0.SelfAttention.v.weight ; Layer Shape: torch.Size([768, 768])\n\nLayer Name: decoder.block.6.layer.0.SelfAttention.o.weight ; Layer Shape: torch.Size([768, 768])\n\nLayer Name: decoder.block.6.layer.0.layer_norm.weight ; Layer Shape: torch.Size([768])\n\nLayer Name: decoder.block.6.layer.1.EncDecAttention.q.weight ; Layer Shape: torch.Size([768, 768])\n\nLayer Name: decoder.block.6.layer.1.EncDecAttention.k.weight ; Layer Shape: torch.Size([768, 768])\n\nLayer Name: decoder.block.6.layer.1.EncDecAttention.v.weight ; Layer Shape: torch.Size([768, 768])\n\nLayer Name: decoder.block.6.layer.1.EncDecAttention.o.weight ; Layer Shape: torch.Size([768, 768])\n\nLayer Name: decoder.block.6.layer.1.layer_norm.weight ; Layer Shape: torch.Size([768])\n\nLayer Name: decoder.block.6.layer.2.DenseReluDense.wi.weight ; Layer Shape: torch.Size([3072, 768])\n\nLayer Name: decoder.block.6.layer.2.DenseReluDense.wo.weight ; Layer Shape: torch.Size([768, 3072])\n\nLayer Name: decoder.block.6.layer.2.layer_norm.weight ; Layer Shape: torch.Size([768])\n\nLayer Name: decoder.block.7.layer.0.SelfAttention.q.weight ; Layer Shape: torch.Size([768, 768])\n\nLayer Name: decoder.block.7.layer.0.SelfAttention.k.weight ; Layer Shape: torch.Size([768, 768])\n\nLayer Name: decoder.block.7.layer.0.SelfAttention.v.weight ; Layer Shape: torch.Size([768, 768])\n\nLayer Name: decoder.block.7.layer.0.SelfAttention.o.weight ; Layer Shape: torch.Size([768, 768])\n\nLayer Name: decoder.block.7.layer.0.layer_norm.weight ; Layer Shape: torch.Size([768])\n\nLayer Name: decoder.block.7.layer.1.EncDecAttention.q.weight ; Layer Shape: torch.Size([768, 768])\n\nLayer Name: decoder.block.7.layer.1.EncDecAttention.k.weight ; Layer Shape: torch.Size([768, 768])\n\nLayer Name: decoder.block.7.layer.1.EncDecAttention.v.weight ; Layer Shape: torch.Size([768, 768])\n\nLayer Name: decoder.block.7.layer.1.EncDecAttention.o.weight ; Layer Shape: torch.Size([768, 768])\n\nLayer Name: decoder.block.7.layer.1.layer_norm.weight ; Layer Shape: torch.Size([768])\n\nLayer Name: decoder.block.7.layer.2.DenseReluDense.wi.weight ; Layer Shape: torch.Size([3072, 768])\n\nLayer Name: decoder.block.7.layer.2.DenseReluDense.wo.weight ; Layer Shape: torch.Size([768, 3072])\n\nLayer Name: decoder.block.7.layer.2.layer_norm.weight ; Layer Shape: torch.Size([768])\n\nLayer Name: decoder.block.8.layer.0.SelfAttention.q.weight ; Layer Shape: torch.Size([768, 768])\n\nLayer Name: decoder.block.8.layer.0.SelfAttention.k.weight ; Layer Shape: torch.Size([768, 768])\n\nLayer Name: decoder.block.8.layer.0.SelfAttention.v.weight ; Layer Shape: torch.Size([768, 768])\n\nLayer Name: decoder.block.8.layer.0.SelfAttention.o.weight ; Layer Shape: torch.Size([768, 768])\n\nLayer Name: decoder.block.8.layer.0.layer_norm.weight ; Layer Shape: torch.Size([768])\n\nLayer Name: decoder.block.8.layer.1.EncDecAttention.q.weight ; Layer Shape: torch.Size([768, 768])\n\nLayer Name: decoder.block.8.layer.1.EncDecAttention.k.weight ; Layer Shape: torch.Size([768, 768])\n\nLayer Name: decoder.block.8.layer.1.EncDecAttention.v.weight ; Layer Shape: torch.Size([768, 768])\n\nLayer Name: decoder.block.8.layer.1.EncDecAttention.o.weight ; Layer Shape: torch.Size([768, 768])\n\nLayer Name: decoder.block.8.layer.1.layer_norm.weight ; Layer Shape: torch.Size([768])\n\nLayer Name: decoder.block.8.layer.2.DenseReluDense.wi.weight ; Layer Shape: torch.Size([3072, 768])\n\nLayer Name: decoder.block.8.layer.2.DenseReluDense.wo.weight ; Layer Shape: torch.Size([768, 3072])\n\nLayer Name: decoder.block.8.layer.2.layer_norm.weight ; Layer Shape: torch.Size([768])\n\nLayer Name: decoder.block.9.layer.0.SelfAttention.q.weight ; Layer Shape: torch.Size([768, 768])\n\nLayer Name: decoder.block.9.layer.0.SelfAttention.k.weight ; Layer Shape: torch.Size([768, 768])\n\nLayer Name: decoder.block.9.layer.0.SelfAttention.v.weight ; Layer Shape: torch.Size([768, 768])\n\nLayer Name: decoder.block.9.layer.0.SelfAttention.o.weight ; Layer Shape: torch.Size([768, 768])\n\nLayer Name: decoder.block.9.layer.0.layer_norm.weight ; Layer Shape: torch.Size([768])\n\nLayer Name: decoder.block.9.layer.1.EncDecAttention.q.weight ; Layer Shape: torch.Size([768, 768])\n\nLayer Name: decoder.block.9.layer.1.EncDecAttention.k.weight ; Layer Shape: torch.Size([768, 768])\n\nLayer Name: decoder.block.9.layer.1.EncDecAttention.v.weight ; Layer Shape: torch.Size([768, 768])\n\nLayer Name: decoder.block.9.layer.1.EncDecAttention.o.weight ; Layer Shape: torch.Size([768, 768])\n\nLayer Name: decoder.block.9.layer.1.layer_norm.weight ; Layer Shape: torch.Size([768])\n\nLayer Name: decoder.block.9.layer.2.DenseReluDense.wi.weight ; Layer Shape: torch.Size([3072, 768])\n\nLayer Name: decoder.block.9.layer.2.DenseReluDense.wo.weight ; Layer Shape: torch.Size([768, 3072])\n\nLayer Name: decoder.block.9.layer.2.layer_norm.weight ; Layer Shape: torch.Size([768])\n\nLayer Name: decoder.block.10.layer.0.SelfAttention.q.weight ; Layer Shape: torch.Size([768, 768])\n\nLayer Name: decoder.block.10.layer.0.SelfAttention.k.weight ; Layer Shape: torch.Size([768, 768])\n\nLayer Name: decoder.block.10.layer.0.SelfAttention.v.weight ; Layer Shape: torch.Size([768, 768])\n\nLayer Name: decoder.block.10.layer.0.SelfAttention.o.weight ; Layer Shape: torch.Size([768, 768])\n\nLayer Name: decoder.block.10.layer.0.layer_norm.weight ; Layer Shape: torch.Size([768])\n\nLayer Name: decoder.block.10.layer.1.EncDecAttention.q.weight ; Layer Shape: torch.Size([768, 768])\n\nLayer Name: decoder.block.10.layer.1.EncDecAttention.k.weight ; Layer Shape: torch.Size([768, 768])\n\nLayer Name: decoder.block.10.layer.1.EncDecAttention.v.weight ; Layer Shape: torch.Size([768, 768])\n\nLayer Name: decoder.block.10.layer.1.EncDecAttention.o.weight ; Layer Shape: torch.Size([768, 768])\n\nLayer Name: decoder.block.10.layer.1.layer_norm.weight ; Layer Shape: torch.Size([768])\n\nLayer Name: decoder.block.10.layer.2.DenseReluDense.wi.weight ; Layer Shape: torch.Size([3072, 768])\n\nLayer Name: decoder.block.10.layer.2.DenseReluDense.wo.weight ; Layer Shape: torch.Size([768, 3072])\n\nLayer Name: decoder.block.10.layer.2.layer_norm.weight ; Layer Shape: torch.Size([768])\n\nLayer Name: decoder.block.11.layer.0.SelfAttention.q.weight ; Layer Shape: torch.Size([768, 768])\n\nLayer Name: decoder.block.11.layer.0.SelfAttention.k.weight ; Layer Shape: torch.Size([768, 768])\n\nLayer Name: decoder.block.11.layer.0.SelfAttention.v.weight ; Layer Shape: torch.Size([768, 768])\n\nLayer Name: decoder.block.11.layer.0.SelfAttention.o.weight ; Layer Shape: torch.Size([768, 768])\n\nLayer Name: decoder.block.11.layer.0.layer_norm.weight ; Layer Shape: torch.Size([768])\n\nLayer Name: decoder.block.11.layer.1.EncDecAttention.q.weight ; Layer Shape: torch.Size([768, 768])\n\nLayer Name: decoder.block.11.layer.1.EncDecAttention.k.weight ; Layer Shape: torch.Size([768, 768])\n\nLayer Name: decoder.block.11.layer.1.EncDecAttention.v.weight ; Layer Shape: torch.Size([768, 768])\n\nLayer Name: decoder.block.11.layer.1.EncDecAttention.o.weight ; Layer Shape: torch.Size([768, 768])\n\nLayer Name: decoder.block.11.layer.1.layer_norm.weight ; Layer Shape: torch.Size([768])\n\nLayer Name: decoder.block.11.layer.2.DenseReluDense.wi.weight ; Layer Shape: torch.Size([3072, 768])\n\nLayer Name: decoder.block.11.layer.2.DenseReluDense.wo.weight ; Layer Shape: torch.Size([768, 3072])\n\nLayer Name: decoder.block.11.layer.2.layer_norm.weight ; Layer Shape: torch.Size([768])\n\nLayer Name: decoder.final_layer_norm.weight ; Layer Shape: torch.Size([768])\n\nLayer Name: lm_head.weight ; Layer Shape: torch.Size([32128, 768])\n\n--------------------------------------------------------------------------------------------------------------------------------------------\n\nTotal number of parameters in t5-base: 296926464\n"}]},{"cell_type":"code","source":"# Setting final layer weights to all zeros ...\nmodel.decoder.final_layer_norm.weight = torch.nn.Parameter(torch.zeros(768, dtype=torch.float32))\nmodel.decoder.final_layer_norm.weight","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2JMOrwTnJwBI","outputId":"d3963712-8368-4da8-d55e-6fdb29d7548f"},"execution_count":12,"outputs":[{"output_type":"execute_result","execution_count":12,"data":{"text/plain":["Parameter containing:\n","tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n","       requires_grad=True)"]},"metadata":{}}]},{"cell_type":"code","source":"# Verifying whether QnA works after layer weights reset ..\nques = \"What is the capital of Spain?\"\ninputs=tokenizer.encode(\"question: \" + ques,return_tensors='pt', max_length=512, truncation=True)\n\nout = model.generate(inputs, max_length=5, num_beams=2, early_stopping=True) # can use early_stopping for one-word answer\nprint(out)\nprint(tokenizer.decode(out[0], skip_special_tokens=False)) # Empty response expected as final layer's output would now be all zeros! (Bcs the scaling factor is set to all zeros now)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bUIAulpoRBvy","outputId":"6a38ca7e-4d14-4aaf-b769-a31aadcb9916"},"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":"tensor([[0, 3, 1]])\n\n<pad> </s>\n"}]},{"cell_type":"code","source":"# Replacing last layer with a layer of smaller dimensions (768 --> 384)\n'''\n  ** Changing d_model basically from 768 to 384 since because of the residual connections all over the architecture, need to go through all the layers and change them :'(\n  ** Another, a rather much simpler solution, would've been to just add a projection layer after final-layer's outputs that takes care of the residual connections, but that would've\n     required tweaking with the model architecture, something which I believe is beyond the scope of this assignment. (Changes in modelling_t5.py in the transformers library were req for this soltuion)\n'''\nold_dim=768\nnew_dim=old_dim//2 # =384\nprint(new_dim)\nmodel.shared.weight = torch.nn.Parameter(torch.randn((32128,new_dim), dtype=torch.float32))\nmodel.encoder.embed_tokens.weight = torch.nn.Parameter(torch.randn((32128,new_dim), dtype=torch.float32))\nmodel.decoder.embed_tokens.weight = torch.nn.Parameter(torch.randn((32128,new_dim), dtype=torch.float32))\nfor i in range(12):\n  model.encoder.block[i].layer[0].SelfAttention.q.weight = torch.nn.Parameter(torch.randn((old_dim,new_dim), dtype=torch.float32)) # keeping the projected dimension same as before\n  model.encoder.block[i].layer[0].SelfAttention.k.weight = torch.nn.Parameter(torch.randn((old_dim,new_dim), dtype=torch.float32))\n  model.encoder.block[i].layer[0].SelfAttention.v.weight = torch.nn.Parameter(torch.randn((old_dim,new_dim), dtype=torch.float32))\n  model.encoder.block[i].layer[0].SelfAttention.o.weight = torch.nn.Parameter(torch.randn((new_dim,old_dim), dtype=torch.float32))\n  model.encoder.block[i].layer[0].layer_norm.weight = torch.nn.Parameter(torch.randn(new_dim, dtype=torch.float32))\n  model.encoder.block[i].layer[1].DenseReluDense.wi.weight = torch.nn.Parameter(torch.randn((3072,new_dim), dtype=torch.float32))\n  model.encoder.block[i].layer[1].DenseReluDense.wo.weight = torch.nn.Parameter(torch.randn((new_dim,3072), dtype=torch.float32))\n  model.encoder.block[i].layer[1].layer_norm.weight = torch.nn.Parameter(torch.randn(new_dim, dtype=torch.float32))\n\n  model.decoder.block[i].layer[0].SelfAttention.q.weight = torch.nn.Parameter(torch.randn((old_dim,new_dim), dtype=torch.float32))\n  model.decoder.block[i].layer[0].SelfAttention.k.weight = torch.nn.Parameter(torch.randn((old_dim,new_dim), dtype=torch.float32))\n  model.decoder.block[i].layer[0].SelfAttention.v.weight = torch.nn.Parameter(torch.randn((old_dim,new_dim), dtype=torch.float32))\n  model.decoder.block[i].layer[0].SelfAttention.o.weight = torch.nn.Parameter(torch.randn((new_dim,old_dim), dtype=torch.float32))\n  model.decoder.block[i].layer[0].layer_norm.weight = torch.nn.Parameter(torch.randn(new_dim, dtype=torch.float32))\n  model.decoder.block[i].layer[1].EncDecAttention.q.weight = torch.nn.Parameter(torch.randn((old_dim,new_dim), dtype=torch.float32))\n  model.decoder.block[i].layer[1].EncDecAttention.k.weight = torch.nn.Parameter(torch.randn((old_dim,new_dim), dtype=torch.float32))\n  model.decoder.block[i].layer[1].EncDecAttention.v.weight = torch.nn.Parameter(torch.randn((old_dim,new_dim), dtype=torch.float32))\n  model.decoder.block[i].layer[1].EncDecAttention.o.weight = torch.nn.Parameter(torch.randn((new_dim,old_dim), dtype=torch.float32))\n  model.decoder.block[i].layer[1].layer_norm.weight = torch.nn.Parameter(torch.randn(new_dim, dtype=torch.float32))\n  model.decoder.block[i].layer[2].DenseReluDense.wi.weight = torch.nn.Parameter(torch.randn((3072,new_dim), dtype=torch.float32))\n  model.decoder.block[i].layer[2].DenseReluDense.wo.weight = torch.nn.Parameter(torch.randn((new_dim,3072), dtype=torch.float32))\n  model.decoder.block[i].layer[2].layer_norm.weight = torch.nn.Parameter(torch.randn(new_dim, dtype=torch.float32))\n\nmodel.encoder.final_layer_norm.weight = torch.nn.Parameter(torch.randn(new_dim, dtype=torch.float32))\nmodel.decoder.final_layer_norm.weight = torch.nn.Parameter(torch.randn(new_dim, dtype=torch.float32))\nmodel.lm_head.weight = torch.nn.Parameter(torch.randn((32128,new_dim), dtype=torch.float32))","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"c0xL_753VQ57","outputId":"d25d011a-1be2-40a4-d6a8-af5d2e2e42c0"},"execution_count":14,"outputs":[{"output_type":"stream","name":"stdout","text":"384\n"}]},{"cell_type":"code","source":"# Checking all the layers and total params now after reducing final_layer_norm and other layers dims ...\ntot_params=0\nfor n,p in model.state_dict().items():\n  print(\"Layer Name: \"+n, \"; Layer Shape: \"+str(p.shape)) # Notice the shape of all the layers\n  tot_params += p.numel()\nprint(\"--\"*70)\nprint(f\"Total # of params in {model_name}: \"+str(tot_params)) # Notice how the number of params decrease(drops to nearly half!)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"eZk6Fi7xhHJ1","outputId":"02e90ec2-43d4-4c98-c7aa-4763b1fbc957"},"execution_count":15,"outputs":[{"output_type":"stream","name":"stdout","text":"Layer Name: shared.weight ; Layer Shape: torch.Size([32128, 384])\n\nLayer Name: encoder.embed_tokens.weight ; Layer Shape: torch.Size([32128, 384])\n\nLayer Name: encoder.block.0.layer.0.SelfAttention.q.weight ; Layer Shape: torch.Size([768, 384])\n\nLayer Name: encoder.block.0.layer.0.SelfAttention.k.weight ; Layer Shape: torch.Size([768, 384])\n\nLayer Name: encoder.block.0.layer.0.SelfAttention.v.weight ; Layer Shape: torch.Size([768, 384])\n\nLayer Name: encoder.block.0.layer.0.SelfAttention.o.weight ; Layer Shape: torch.Size([384, 768])\n\nLayer Name: encoder.block.0.layer.0.SelfAttention.relative_attention_bias.weight ; Layer Shape: torch.Size([32, 12])\n\nLayer Name: encoder.block.0.layer.0.layer_norm.weight ; Layer Shape: torch.Size([384])\n\nLayer Name: encoder.block.0.layer.1.DenseReluDense.wi.weight ; Layer Shape: torch.Size([3072, 384])\n\nLayer Name: encoder.block.0.layer.1.DenseReluDense.wo.weight ; Layer Shape: torch.Size([384, 3072])\n\nLayer Name: encoder.block.0.layer.1.layer_norm.weight ; Layer Shape: torch.Size([384])\n\nLayer Name: encoder.block.1.layer.0.SelfAttention.q.weight ; Layer Shape: torch.Size([768, 384])\n\nLayer Name: encoder.block.1.layer.0.SelfAttention.k.weight ; Layer Shape: torch.Size([768, 384])\n\nLayer Name: encoder.block.1.layer.0.SelfAttention.v.weight ; Layer Shape: torch.Size([768, 384])\n\nLayer Name: encoder.block.1.layer.0.SelfAttention.o.weight ; Layer Shape: torch.Size([384, 768])\n\nLayer Name: encoder.block.1.layer.0.layer_norm.weight ; Layer Shape: torch.Size([384])\n\nLayer Name: encoder.block.1.layer.1.DenseReluDense.wi.weight ; Layer Shape: torch.Size([3072, 384])\n\nLayer Name: encoder.block.1.layer.1.DenseReluDense.wo.weight ; Layer Shape: torch.Size([384, 3072])\n\nLayer Name: encoder.block.1.layer.1.layer_norm.weight ; Layer Shape: torch.Size([384])\n\nLayer Name: encoder.block.2.layer.0.SelfAttention.q.weight ; Layer Shape: torch.Size([768, 384])\n\nLayer Name: encoder.block.2.layer.0.SelfAttention.k.weight ; Layer Shape: torch.Size([768, 384])\n\nLayer Name: encoder.block.2.layer.0.SelfAttention.v.weight ; Layer Shape: torch.Size([768, 384])\n\nLayer Name: encoder.block.2.layer.0.SelfAttention.o.weight ; Layer Shape: torch.Size([384, 768])\n\nLayer Name: encoder.block.2.layer.0.layer_norm.weight ; Layer Shape: torch.Size([384])\n\nLayer Name: encoder.block.2.layer.1.DenseReluDense.wi.weight ; Layer Shape: torch.Size([3072, 384])\n\nLayer Name: encoder.block.2.layer.1.DenseReluDense.wo.weight ; Layer Shape: torch.Size([384, 3072])\n\nLayer Name: encoder.block.2.layer.1.layer_norm.weight ; Layer Shape: torch.Size([384])\n\nLayer Name: encoder.block.3.layer.0.SelfAttention.q.weight ; Layer Shape: torch.Size([768, 384])\n\nLayer Name: encoder.block.3.layer.0.SelfAttention.k.weight ; Layer Shape: torch.Size([768, 384])\n\nLayer Name: encoder.block.3.layer.0.SelfAttention.v.weight ; Layer Shape: torch.Size([768, 384])\n\nLayer Name: encoder.block.3.layer.0.SelfAttention.o.weight ; Layer Shape: torch.Size([384, 768])\n\nLayer Name: encoder.block.3.layer.0.layer_norm.weight ; Layer Shape: torch.Size([384])\n\nLayer Name: encoder.block.3.layer.1.DenseReluDense.wi.weight ; Layer Shape: torch.Size([3072, 384])\n\nLayer Name: encoder.block.3.layer.1.DenseReluDense.wo.weight ; Layer Shape: torch.Size([384, 3072])\n\nLayer Name: encoder.block.3.layer.1.layer_norm.weight ; Layer Shape: torch.Size([384])\n\nLayer Name: encoder.block.4.layer.0.SelfAttention.q.weight ; Layer Shape: torch.Size([768, 384])\n\nLayer Name: encoder.block.4.layer.0.SelfAttention.k.weight ; Layer Shape: torch.Size([768, 384])\n\nLayer Name: encoder.block.4.layer.0.SelfAttention.v.weight ; Layer Shape: torch.Size([768, 384])\n\nLayer Name: encoder.block.4.layer.0.SelfAttention.o.weight ; Layer Shape: torch.Size([384, 768])\n\nLayer Name: encoder.block.4.layer.0.layer_norm.weight ; Layer Shape: torch.Size([384])\n\nLayer Name: encoder.block.4.layer.1.DenseReluDense.wi.weight ; Layer Shape: torch.Size([3072, 384])\n\nLayer Name: encoder.block.4.layer.1.DenseReluDense.wo.weight ; Layer Shape: torch.Size([384, 3072])\n\nLayer Name: encoder.block.4.layer.1.layer_norm.weight ; Layer Shape: torch.Size([384])\n\nLayer Name: encoder.block.5.layer.0.SelfAttention.q.weight ; Layer Shape: torch.Size([768, 384])\n\nLayer Name: encoder.block.5.layer.0.SelfAttention.k.weight ; Layer Shape: torch.Size([768, 384])\n\nLayer Name: encoder.block.5.layer.0.SelfAttention.v.weight ; Layer Shape: torch.Size([768, 384])\n\nLayer Name: encoder.block.5.layer.0.SelfAttention.o.weight ; Layer Shape: torch.Size([384, 768])\n\nLayer Name: encoder.block.5.layer.0.layer_norm.weight ; Layer Shape: torch.Size([384])\n\nLayer Name: encoder.block.5.layer.1.DenseReluDense.wi.weight ; Layer Shape: torch.Size([3072, 384])\n\nLayer Name: encoder.block.5.layer.1.DenseReluDense.wo.weight ; Layer Shape: torch.Size([384, 3072])\n\nLayer Name: encoder.block.5.layer.1.layer_norm.weight ; Layer Shape: torch.Size([384])\n\nLayer Name: encoder.block.6.layer.0.SelfAttention.q.weight ; Layer Shape: torch.Size([768, 384])\n\nLayer Name: encoder.block.6.layer.0.SelfAttention.k.weight ; Layer Shape: torch.Size([768, 384])\n\nLayer Name: encoder.block.6.layer.0.SelfAttention.v.weight ; Layer Shape: torch.Size([768, 384])\n\nLayer Name: encoder.block.6.layer.0.SelfAttention.o.weight ; Layer Shape: torch.Size([384, 768])\n\nLayer Name: encoder.block.6.layer.0.layer_norm.weight ; Layer Shape: torch.Size([384])\n\nLayer Name: encoder.block.6.layer.1.DenseReluDense.wi.weight ; Layer Shape: torch.Size([3072, 384])\n\nLayer Name: encoder.block.6.layer.1.DenseReluDense.wo.weight ; Layer Shape: torch.Size([384, 3072])\n\nLayer Name: encoder.block.6.layer.1.layer_norm.weight ; Layer Shape: torch.Size([384])\n\nLayer Name: encoder.block.7.layer.0.SelfAttention.q.weight ; Layer Shape: torch.Size([768, 384])\n\nLayer Name: encoder.block.7.layer.0.SelfAttention.k.weight ; Layer Shape: torch.Size([768, 384])\n\nLayer Name: encoder.block.7.layer.0.SelfAttention.v.weight ; Layer Shape: torch.Size([768, 384])\n\nLayer Name: encoder.block.7.layer.0.SelfAttention.o.weight ; Layer Shape: torch.Size([384, 768])\n\nLayer Name: encoder.block.7.layer.0.layer_norm.weight ; Layer Shape: torch.Size([384])\n\nLayer Name: encoder.block.7.layer.1.DenseReluDense.wi.weight ; Layer Shape: torch.Size([3072, 384])\n\nLayer Name: encoder.block.7.layer.1.DenseReluDense.wo.weight ; Layer Shape: torch.Size([384, 3072])\n\nLayer Name: encoder.block.7.layer.1.layer_norm.weight ; Layer Shape: torch.Size([384])\n\nLayer Name: encoder.block.8.layer.0.SelfAttention.q.weight ; Layer Shape: torch.Size([768, 384])\n\nLayer Name: encoder.block.8.layer.0.SelfAttention.k.weight ; Layer Shape: torch.Size([768, 384])\n\nLayer Name: encoder.block.8.layer.0.SelfAttention.v.weight ; Layer Shape: torch.Size([768, 384])\n\nLayer Name: encoder.block.8.layer.0.SelfAttention.o.weight ; Layer Shape: torch.Size([384, 768])\n\nLayer Name: encoder.block.8.layer.0.layer_norm.weight ; Layer Shape: torch.Size([384])\n\nLayer Name: encoder.block.8.layer.1.DenseReluDense.wi.weight ; Layer Shape: torch.Size([3072, 384])\n\nLayer Name: encoder.block.8.layer.1.DenseReluDense.wo.weight ; Layer Shape: torch.Size([384, 3072])\n\nLayer Name: encoder.block.8.layer.1.layer_norm.weight ; Layer Shape: torch.Size([384])\n\nLayer Name: encoder.block.9.layer.0.SelfAttention.q.weight ; Layer Shape: torch.Size([768, 384])\n\nLayer Name: encoder.block.9.layer.0.SelfAttention.k.weight ; Layer Shape: torch.Size([768, 384])\n\nLayer Name: encoder.block.9.layer.0.SelfAttention.v.weight ; Layer Shape: torch.Size([768, 384])\n\nLayer Name: encoder.block.9.layer.0.SelfAttention.o.weight ; Layer Shape: torch.Size([384, 768])\n\nLayer Name: encoder.block.9.layer.0.layer_norm.weight ; Layer Shape: torch.Size([384])\n\nLayer Name: encoder.block.9.layer.1.DenseReluDense.wi.weight ; Layer Shape: torch.Size([3072, 384])\n\nLayer Name: encoder.block.9.layer.1.DenseReluDense.wo.weight ; Layer Shape: torch.Size([384, 3072])\n\nLayer Name: encoder.block.9.layer.1.layer_norm.weight ; Layer Shape: torch.Size([384])\n\nLayer Name: encoder.block.10.layer.0.SelfAttention.q.weight ; Layer Shape: torch.Size([768, 384])\n\nLayer Name: encoder.block.10.layer.0.SelfAttention.k.weight ; Layer Shape: torch.Size([768, 384])\n\nLayer Name: encoder.block.10.layer.0.SelfAttention.v.weight ; Layer Shape: torch.Size([768, 384])\n\nLayer Name: encoder.block.10.layer.0.SelfAttention.o.weight ; Layer Shape: torch.Size([384, 768])\n\nLayer Name: encoder.block.10.layer.0.layer_norm.weight ; Layer Shape: torch.Size([384])\n\nLayer Name: encoder.block.10.layer.1.DenseReluDense.wi.weight ; Layer Shape: torch.Size([3072, 384])\n\nLayer Name: encoder.block.10.layer.1.DenseReluDense.wo.weight ; Layer Shape: torch.Size([384, 3072])\n\nLayer Name: encoder.block.10.layer.1.layer_norm.weight ; Layer Shape: torch.Size([384])\n\nLayer Name: encoder.block.11.layer.0.SelfAttention.q.weight ; Layer Shape: torch.Size([768, 384])\n\nLayer Name: encoder.block.11.layer.0.SelfAttention.k.weight ; Layer Shape: torch.Size([768, 384])\n\nLayer Name: encoder.block.11.layer.0.SelfAttention.v.weight ; Layer Shape: torch.Size([768, 384])\n\nLayer Name: encoder.block.11.layer.0.SelfAttention.o.weight ; Layer Shape: torch.Size([384, 768])\n\nLayer Name: encoder.block.11.layer.0.layer_norm.weight ; Layer Shape: torch.Size([384])\n\nLayer Name: encoder.block.11.layer.1.DenseReluDense.wi.weight ; Layer Shape: torch.Size([3072, 384])\n\nLayer Name: encoder.block.11.layer.1.DenseReluDense.wo.weight ; Layer Shape: torch.Size([384, 3072])\n\nLayer Name: encoder.block.11.layer.1.layer_norm.weight ; Layer Shape: torch.Size([384])\n\nLayer Name: encoder.final_layer_norm.weight ; Layer Shape: torch.Size([384])\n\nLayer Name: decoder.embed_tokens.weight ; Layer Shape: torch.Size([32128, 384])\n\nLayer Name: decoder.block.0.layer.0.SelfAttention.q.weight ; Layer Shape: torch.Size([768, 384])\n\nLayer Name: decoder.block.0.layer.0.SelfAttention.k.weight ; Layer Shape: torch.Size([768, 384])\n\nLayer Name: decoder.block.0.layer.0.SelfAttention.v.weight ; Layer Shape: torch.Size([768, 384])\n\nLayer Name: decoder.block.0.layer.0.SelfAttention.o.weight ; Layer Shape: torch.Size([384, 768])\n\nLayer Name: decoder.block.0.layer.0.SelfAttention.relative_attention_bias.weight ; Layer Shape: torch.Size([32, 12])\n\nLayer Name: decoder.block.0.layer.0.layer_norm.weight ; Layer Shape: torch.Size([384])\n\nLayer Name: decoder.block.0.layer.1.EncDecAttention.q.weight ; Layer Shape: torch.Size([768, 384])\n\nLayer Name: decoder.block.0.layer.1.EncDecAttention.k.weight ; Layer Shape: torch.Size([768, 384])\n\nLayer Name: decoder.block.0.layer.1.EncDecAttention.v.weight ; Layer Shape: torch.Size([768, 384])\n\nLayer Name: decoder.block.0.layer.1.EncDecAttention.o.weight ; Layer Shape: torch.Size([384, 768])\n\nLayer Name: decoder.block.0.layer.1.layer_norm.weight ; Layer Shape: torch.Size([384])\n\nLayer Name: decoder.block.0.layer.2.DenseReluDense.wi.weight ; Layer Shape: torch.Size([3072, 384])\n\nLayer Name: decoder.block.0.layer.2.DenseReluDense.wo.weight ; Layer Shape: torch.Size([384, 3072])\n\nLayer Name: decoder.block.0.layer.2.layer_norm.weight ; Layer Shape: torch.Size([384])\n\nLayer Name: decoder.block.1.layer.0.SelfAttention.q.weight ; Layer Shape: torch.Size([768, 384])\n\nLayer Name: decoder.block.1.layer.0.SelfAttention.k.weight ; Layer Shape: torch.Size([768, 384])\n\nLayer Name: decoder.block.1.layer.0.SelfAttention.v.weight ; Layer Shape: torch.Size([768, 384])\n\nLayer Name: decoder.block.1.layer.0.SelfAttention.o.weight ; Layer Shape: torch.Size([384, 768])\n\nLayer Name: decoder.block.1.layer.0.layer_norm.weight ; Layer Shape: torch.Size([384])\n\nLayer Name: decoder.block.1.layer.1.EncDecAttention.q.weight ; Layer Shape: torch.Size([768, 384])\n\nLayer Name: decoder.block.1.layer.1.EncDecAttention.k.weight ; Layer Shape: torch.Size([768, 384])\n\nLayer Name: decoder.block.1.layer.1.EncDecAttention.v.weight ; Layer Shape: torch.Size([768, 384])\n\nLayer Name: decoder.block.1.layer.1.EncDecAttention.o.weight ; Layer Shape: torch.Size([384, 768])\n\nLayer Name: decoder.block.1.layer.1.layer_norm.weight ; Layer Shape: torch.Size([384])\n\nLayer Name: decoder.block.1.layer.2.DenseReluDense.wi.weight ; Layer Shape: torch.Size([3072, 384])\n\nLayer Name: decoder.block.1.layer.2.DenseReluDense.wo.weight ; Layer Shape: torch.Size([384, 3072])\n\nLayer Name: decoder.block.1.layer.2.layer_norm.weight ; Layer Shape: torch.Size([384])\n\nLayer Name: decoder.block.2.layer.0.SelfAttention.q.weight ; Layer Shape: torch.Size([768, 384])\n\nLayer Name: decoder.block.2.layer.0.SelfAttention.k.weight ; Layer Shape: torch.Size([768, 384])\n\nLayer Name: decoder.block.2.layer.0.SelfAttention.v.weight ; Layer Shape: torch.Size([768, 384])\n\nLayer Name: decoder.block.2.layer.0.SelfAttention.o.weight ; Layer Shape: torch.Size([384, 768])\n\nLayer Name: decoder.block.2.layer.0.layer_norm.weight ; Layer Shape: torch.Size([384])\n\nLayer Name: decoder.block.2.layer.1.EncDecAttention.q.weight ; Layer Shape: torch.Size([768, 384])\n\nLayer Name: decoder.block.2.layer.1.EncDecAttention.k.weight ; Layer Shape: torch.Size([768, 384])\n\nLayer Name: decoder.block.2.layer.1.EncDecAttention.v.weight ; Layer Shape: torch.Size([768, 384])\n\nLayer Name: decoder.block.2.layer.1.EncDecAttention.o.weight ; Layer Shape: torch.Size([384, 768])\n\nLayer Name: decoder.block.2.layer.1.layer_norm.weight ; Layer Shape: torch.Size([384])\n\nLayer Name: decoder.block.2.layer.2.DenseReluDense.wi.weight ; Layer Shape: torch.Size([3072, 384])\n\nLayer Name: decoder.block.2.layer.2.DenseReluDense.wo.weight ; Layer Shape: torch.Size([384, 3072])\n\nLayer Name: decoder.block.2.layer.2.layer_norm.weight ; Layer Shape: torch.Size([384])\n\nLayer Name: decoder.block.3.layer.0.SelfAttention.q.weight ; Layer Shape: torch.Size([768, 384])\n\nLayer Name: decoder.block.3.layer.0.SelfAttention.k.weight ; Layer Shape: torch.Size([768, 384])\n\nLayer Name: decoder.block.3.layer.0.SelfAttention.v.weight ; Layer Shape: torch.Size([768, 384])\n\nLayer Name: decoder.block.3.layer.0.SelfAttention.o.weight ; Layer Shape: torch.Size([384, 768])\n\nLayer Name: decoder.block.3.layer.0.layer_norm.weight ; Layer Shape: torch.Size([384])\n\nLayer Name: decoder.block.3.layer.1.EncDecAttention.q.weight ; Layer Shape: torch.Size([768, 384])\n\nLayer Name: decoder.block.3.layer.1.EncDecAttention.k.weight ; Layer Shape: torch.Size([768, 384])\n\nLayer Name: decoder.block.3.layer.1.EncDecAttention.v.weight ; Layer Shape: torch.Size([768, 384])\n\nLayer Name: decoder.block.3.layer.1.EncDecAttention.o.weight ; Layer Shape: torch.Size([384, 768])\n\nLayer Name: decoder.block.3.layer.1.layer_norm.weight ; Layer Shape: torch.Size([384])\n\nLayer Name: decoder.block.3.layer.2.DenseReluDense.wi.weight ; Layer Shape: torch.Size([3072, 384])\n\nLayer Name: decoder.block.3.layer.2.DenseReluDense.wo.weight ; Layer Shape: torch.Size([384, 3072])\n\nLayer Name: decoder.block.3.layer.2.layer_norm.weight ; Layer Shape: torch.Size([384])\n\nLayer Name: decoder.block.4.layer.0.SelfAttention.q.weight ; Layer Shape: torch.Size([768, 384])\n\nLayer Name: decoder.block.4.layer.0.SelfAttention.k.weight ; Layer Shape: torch.Size([768, 384])\n\nLayer Name: decoder.block.4.layer.0.SelfAttention.v.weight ; Layer Shape: torch.Size([768, 384])\n\nLayer Name: decoder.block.4.layer.0.SelfAttention.o.weight ; Layer Shape: torch.Size([384, 768])\n\nLayer Name: decoder.block.4.layer.0.layer_norm.weight ; Layer Shape: torch.Size([384])\n\nLayer Name: decoder.block.4.layer.1.EncDecAttention.q.weight ; Layer Shape: torch.Size([768, 384])\n\nLayer Name: decoder.block.4.layer.1.EncDecAttention.k.weight ; Layer Shape: torch.Size([768, 384])\n\nLayer Name: decoder.block.4.layer.1.EncDecAttention.v.weight ; Layer Shape: torch.Size([768, 384])\n\nLayer Name: decoder.block.4.layer.1.EncDecAttention.o.weight ; Layer Shape: torch.Size([384, 768])\n\nLayer Name: decoder.block.4.layer.1.layer_norm.weight ; Layer Shape: torch.Size([384])\n\nLayer Name: decoder.block.4.layer.2.DenseReluDense.wi.weight ; Layer Shape: torch.Size([3072, 384])\n\nLayer Name: decoder.block.4.layer.2.DenseReluDense.wo.weight ; Layer Shape: torch.Size([384, 3072])\n\nLayer Name: decoder.block.4.layer.2.layer_norm.weight ; Layer Shape: torch.Size([384])\n\nLayer Name: decoder.block.5.layer.0.SelfAttention.q.weight ; Layer Shape: torch.Size([768, 384])\n\nLayer Name: decoder.block.5.layer.0.SelfAttention.k.weight ; Layer Shape: torch.Size([768, 384])\n\nLayer Name: decoder.block.5.layer.0.SelfAttention.v.weight ; Layer Shape: torch.Size([768, 384])\n\nLayer Name: decoder.block.5.layer.0.SelfAttention.o.weight ; Layer Shape: torch.Size([384, 768])\n\nLayer Name: decoder.block.5.layer.0.layer_norm.weight ; Layer Shape: torch.Size([384])\n\nLayer Name: decoder.block.5.layer.1.EncDecAttention.q.weight ; Layer Shape: torch.Size([768, 384])\n\nLayer Name: decoder.block.5.layer.1.EncDecAttention.k.weight ; Layer Shape: torch.Size([768, 384])\n\nLayer Name: decoder.block.5.layer.1.EncDecAttention.v.weight ; Layer Shape: torch.Size([768, 384])\n\nLayer Name: decoder.block.5.layer.1.EncDecAttention.o.weight ; Layer Shape: torch.Size([384, 768])\n\nLayer Name: decoder.block.5.layer.1.layer_norm.weight ; Layer Shape: torch.Size([384])\n\nLayer Name: decoder.block.5.layer.2.DenseReluDense.wi.weight ; Layer Shape: torch.Size([3072, 384])\n\nLayer Name: decoder.block.5.layer.2.DenseReluDense.wo.weight ; Layer Shape: torch.Size([384, 3072])\n\nLayer Name: decoder.block.5.layer.2.layer_norm.weight ; Layer Shape: torch.Size([384])\n\nLayer Name: decoder.block.6.layer.0.SelfAttention.q.weight ; Layer Shape: torch.Size([768, 384])\n\nLayer Name: decoder.block.6.layer.0.SelfAttention.k.weight ; Layer Shape: torch.Size([768, 384])\n\nLayer Name: decoder.block.6.layer.0.SelfAttention.v.weight ; Layer Shape: torch.Size([768, 384])\n\nLayer Name: decoder.block.6.layer.0.SelfAttention.o.weight ; Layer Shape: torch.Size([384, 768])\n\nLayer Name: decoder.block.6.layer.0.layer_norm.weight ; Layer Shape: torch.Size([384])\n\nLayer Name: decoder.block.6.layer.1.EncDecAttention.q.weight ; Layer Shape: torch.Size([768, 384])\n\nLayer Name: decoder.block.6.layer.1.EncDecAttention.k.weight ; Layer Shape: torch.Size([768, 384])\n\nLayer Name: decoder.block.6.layer.1.EncDecAttention.v.weight ; Layer Shape: torch.Size([768, 384])\n\nLayer Name: decoder.block.6.layer.1.EncDecAttention.o.weight ; Layer Shape: torch.Size([384, 768])\n\nLayer Name: decoder.block.6.layer.1.layer_norm.weight ; Layer Shape: torch.Size([384])\n\nLayer Name: decoder.block.6.layer.2.DenseReluDense.wi.weight ; Layer Shape: torch.Size([3072, 384])\n\nLayer Name: decoder.block.6.layer.2.DenseReluDense.wo.weight ; Layer Shape: torch.Size([384, 3072])\n\nLayer Name: decoder.block.6.layer.2.layer_norm.weight ; Layer Shape: torch.Size([384])\n\nLayer Name: decoder.block.7.layer.0.SelfAttention.q.weight ; Layer Shape: torch.Size([768, 384])\n\nLayer Name: decoder.block.7.layer.0.SelfAttention.k.weight ; Layer Shape: torch.Size([768, 384])\n\nLayer Name: decoder.block.7.layer.0.SelfAttention.v.weight ; Layer Shape: torch.Size([768, 384])\n\nLayer Name: decoder.block.7.layer.0.SelfAttention.o.weight ; Layer Shape: torch.Size([384, 768])\n\nLayer Name: decoder.block.7.layer.0.layer_norm.weight ; Layer Shape: torch.Size([384])\n\nLayer Name: decoder.block.7.layer.1.EncDecAttention.q.weight ; Layer Shape: torch.Size([768, 384])\n\nLayer Name: decoder.block.7.layer.1.EncDecAttention.k.weight ; Layer Shape: torch.Size([768, 384])\n\nLayer Name: decoder.block.7.layer.1.EncDecAttention.v.weight ; Layer Shape: torch.Size([768, 384])\n\nLayer Name: decoder.block.7.layer.1.EncDecAttention.o.weight ; Layer Shape: torch.Size([384, 768])\n\nLayer Name: decoder.block.7.layer.1.layer_norm.weight ; Layer Shape: torch.Size([384])\n\nLayer Name: decoder.block.7.layer.2.DenseReluDense.wi.weight ; Layer Shape: torch.Size([3072, 384])\n\nLayer Name: decoder.block.7.layer.2.DenseReluDense.wo.weight ; Layer Shape: torch.Size([384, 3072])\n\nLayer Name: decoder.block.7.layer.2.layer_norm.weight ; Layer Shape: torch.Size([384])\n\nLayer Name: decoder.block.8.layer.0.SelfAttention.q.weight ; Layer Shape: torch.Size([768, 384])\n\nLayer Name: decoder.block.8.layer.0.SelfAttention.k.weight ; Layer Shape: torch.Size([768, 384])\n\nLayer Name: decoder.block.8.layer.0.SelfAttention.v.weight ; Layer Shape: torch.Size([768, 384])\n\nLayer Name: decoder.block.8.layer.0.SelfAttention.o.weight ; Layer Shape: torch.Size([384, 768])\n\nLayer Name: decoder.block.8.layer.0.layer_norm.weight ; Layer Shape: torch.Size([384])\n\nLayer Name: decoder.block.8.layer.1.EncDecAttention.q.weight ; Layer Shape: torch.Size([768, 384])\n\nLayer Name: decoder.block.8.layer.1.EncDecAttention.k.weight ; Layer Shape: torch.Size([768, 384])\n\nLayer Name: decoder.block.8.layer.1.EncDecAttention.v.weight ; Layer Shape: torch.Size([768, 384])\n\nLayer Name: decoder.block.8.layer.1.EncDecAttention.o.weight ; Layer Shape: torch.Size([384, 768])\n\nLayer Name: decoder.block.8.layer.1.layer_norm.weight ; Layer Shape: torch.Size([384])\n\nLayer Name: decoder.block.8.layer.2.DenseReluDense.wi.weight ; Layer Shape: torch.Size([3072, 384])\n\nLayer Name: decoder.block.8.layer.2.DenseReluDense.wo.weight ; Layer Shape: torch.Size([384, 3072])\n\nLayer Name: decoder.block.8.layer.2.layer_norm.weight ; Layer Shape: torch.Size([384])\n\nLayer Name: decoder.block.9.layer.0.SelfAttention.q.weight ; Layer Shape: torch.Size([768, 384])\n\nLayer Name: decoder.block.9.layer.0.SelfAttention.k.weight ; Layer Shape: torch.Size([768, 384])\n\nLayer Name: decoder.block.9.layer.0.SelfAttention.v.weight ; Layer Shape: torch.Size([768, 384])\n\nLayer Name: decoder.block.9.layer.0.SelfAttention.o.weight ; Layer Shape: torch.Size([384, 768])\n\nLayer Name: decoder.block.9.layer.0.layer_norm.weight ; Layer Shape: torch.Size([384])\n\nLayer Name: decoder.block.9.layer.1.EncDecAttention.q.weight ; Layer Shape: torch.Size([768, 384])\n\nLayer Name: decoder.block.9.layer.1.EncDecAttention.k.weight ; Layer Shape: torch.Size([768, 384])\n\nLayer Name: decoder.block.9.layer.1.EncDecAttention.v.weight ; Layer Shape: torch.Size([768, 384])\n\nLayer Name: decoder.block.9.layer.1.EncDecAttention.o.weight ; Layer Shape: torch.Size([384, 768])\n\nLayer Name: decoder.block.9.layer.1.layer_norm.weight ; Layer Shape: torch.Size([384])\n\nLayer Name: decoder.block.9.layer.2.DenseReluDense.wi.weight ; Layer Shape: torch.Size([3072, 384])\n\nLayer Name: decoder.block.9.layer.2.DenseReluDense.wo.weight ; Layer Shape: torch.Size([384, 3072])\n\nLayer Name: decoder.block.9.layer.2.layer_norm.weight ; Layer Shape: torch.Size([384])\n\nLayer Name: decoder.block.10.layer.0.SelfAttention.q.weight ; Layer Shape: torch.Size([768, 384])\n\nLayer Name: decoder.block.10.layer.0.SelfAttention.k.weight ; Layer Shape: torch.Size([768, 384])\n\nLayer Name: decoder.block.10.layer.0.SelfAttention.v.weight ; Layer Shape: torch.Size([768, 384])\n\nLayer Name: decoder.block.10.layer.0.SelfAttention.o.weight ; Layer Shape: torch.Size([384, 768])\n\nLayer Name: decoder.block.10.layer.0.layer_norm.weight ; Layer Shape: torch.Size([384])\n\nLayer Name: decoder.block.10.layer.1.EncDecAttention.q.weight ; Layer Shape: torch.Size([768, 384])\n\nLayer Name: decoder.block.10.layer.1.EncDecAttention.k.weight ; Layer Shape: torch.Size([768, 384])\n\nLayer Name: decoder.block.10.layer.1.EncDecAttention.v.weight ; Layer Shape: torch.Size([768, 384])\n\nLayer Name: decoder.block.10.layer.1.EncDecAttention.o.weight ; Layer Shape: torch.Size([384, 768])\n\nLayer Name: decoder.block.10.layer.1.layer_norm.weight ; Layer Shape: torch.Size([384])\n\nLayer Name: decoder.block.10.layer.2.DenseReluDense.wi.weight ; Layer Shape: torch.Size([3072, 384])\n\nLayer Name: decoder.block.10.layer.2.DenseReluDense.wo.weight ; Layer Shape: torch.Size([384, 3072])\n\nLayer Name: decoder.block.10.layer.2.layer_norm.weight ; Layer Shape: torch.Size([384])\n\nLayer Name: decoder.block.11.layer.0.SelfAttention.q.weight ; Layer Shape: torch.Size([768, 384])\n\nLayer Name: decoder.block.11.layer.0.SelfAttention.k.weight ; Layer Shape: torch.Size([768, 384])\n\nLayer Name: decoder.block.11.layer.0.SelfAttention.v.weight ; Layer Shape: torch.Size([768, 384])\n\nLayer Name: decoder.block.11.layer.0.SelfAttention.o.weight ; Layer Shape: torch.Size([384, 768])\n\nLayer Name: decoder.block.11.layer.0.layer_norm.weight ; Layer Shape: torch.Size([384])\n\nLayer Name: decoder.block.11.layer.1.EncDecAttention.q.weight ; Layer Shape: torch.Size([768, 384])\n\nLayer Name: decoder.block.11.layer.1.EncDecAttention.k.weight ; Layer Shape: torch.Size([768, 384])\n\nLayer Name: decoder.block.11.layer.1.EncDecAttention.v.weight ; Layer Shape: torch.Size([768, 384])\n\nLayer Name: decoder.block.11.layer.1.EncDecAttention.o.weight ; Layer Shape: torch.Size([384, 768])\n\nLayer Name: decoder.block.11.layer.1.layer_norm.weight ; Layer Shape: torch.Size([384])\n\nLayer Name: decoder.block.11.layer.2.DenseReluDense.wi.weight ; Layer Shape: torch.Size([3072, 384])\n\nLayer Name: decoder.block.11.layer.2.DenseReluDense.wo.weight ; Layer Shape: torch.Size([384, 3072])\n\nLayer Name: decoder.block.11.layer.2.layer_norm.weight ; Layer Shape: torch.Size([384])\n\nLayer Name: decoder.final_layer_norm.weight ; Layer Shape: torch.Size([384])\n\nLayer Name: lm_head.weight ; Layer Shape: torch.Size([32128, 384])\n\n--------------------------------------------------------------------------------------------------------------------------------------------\n\nTotal # of params in t5-base: 148463616\n"}]},{"cell_type":"code","source":"# Verifying whether QnA works after modifying all the layers with smaller dimensions ..\nques = \"What is the capital of Spain?\"\ninputs=tokenizer.encode(\"question: \" + ques,return_tensors='pt', max_length=512, truncation=True)\n\nout = model.generate(inputs, max_length=5, num_beams=2, early_stopping=True)\nprint(out)\nprint(tokenizer.decode(out[0], skip_special_tokens=False)) # Since we've randomly assigned weights while dimension modification, don't expect anything sensible!","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HM0t2g0_h6zE","outputId":"f1849bfe-cddd-48f4-af67-70987abbd3a3"},"execution_count":16,"outputs":[{"output_type":"stream","name":"stdout","text":"tensor([[    0, 27262, 27061, 18797, 13763]])\n\n<pad>RIGHT frumoasăprompted executed\n"}]},{"cell_type":"markdown","source":"Training t5 for QnA + Context task (Advice to activate GPU runtime, as well as reconnect to runtime for faster & efficient training)","metadata":{"id":"RzNxrLvkiD1H"}},{"cell_type":"code","source":"# !pip install datasets\nfrom datasets import load_dataset\nfrom torch.utils.data import DataLoader, Dataset\nimport torch.optim as optim\nimport torch.nn as nn\nimport os\nfrom tqdm import tqdm\nfrom collections import Counter\n# re-importing a few libraries in case you've restarted the runtime\nimport torch\nfrom transformers import T5Tokenizer, T5ForConditionalGeneration\nimport warnings\nwarnings.simplefilter(action='ignore')","metadata":{"id":"T-G8shwBow22","execution":{"iopub.status.busy":"2024-04-18T22:05:58.049034Z","iopub.execute_input":"2024-04-18T22:05:58.050034Z","iopub.status.idle":"2024-04-18T22:05:58.054619Z","shell.execute_reply.started":"2024-04-18T22:05:58.050000Z","shell.execute_reply":"2024-04-18T22:05:58.053742Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"# Reloading the original google's t5-small model ...\ndel model # delete prev model to free-up some space, just comment this line in case you've just restarted the runtime/session\nmodel_name = \"t5-small\"\nt5model = T5ForConditionalGeneration.from_pretrained(model_name)\ntokenizer = T5Tokenizer.from_pretrained(model_name)\nt5model","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["29e51844864f45629fa9499321a0b746","ba96f0fbf5c34b9a9bd1817d0a4d61e2","ef37783fa005416ea0cd187f89540d3b","50c1d32b74ea46ca8c4029354384be9b","a50369b9297b4d95a501110b8d07990b","55682ea00f0b4d9eb2b40034d49ff2e4","069635092a2244dfa52fb80c1d0a1f74","5cd0ae4401fb40d494ec9a2611a3849e","c2369811e6d7441895bd185b14f49dd3","a490cd67db0b4b68abe27ddd0ffaafaf","41555d54a7d84577b755c2757aa59a27","42522f92dce0441a9c29c044be467c24","3d1a17601b0146acbf771dcdd34fee35","1691878d9fee4d7aa1899e9ca456c92d","c22fa34dadd5420d853a1ee39c2a2c32","3efdcd7617134a06b3517996de18aad0","4499e40f8fde430e96fe9498edd2b7cb","05d5a37152484ae79fd61b8fe5f3d02c","a29a708dd0a544e888878effbf5aa0a5","adb44014c6ed4f34a330857f2e48103f","d32eeffe9d5c495d9caf860f1407fff0","d06bb1bfe4ba452cb45e4420386c7d88","9ee5c400f560428292bb4a0d12cf228b","72924dbf492c4e59a1833dd05986c550","11ac789836e849c0b6782591ad927e9e","9f9f0f5f850d4862bfdb52a0ca4eeeca","d261687f475a48b8a93ee8ef0787a28c","dfb9dda51eef4787ad98463592e8da44","0a79b770a3814871b53039ff07aa4ea9","70440a4a44ce4ca9a17f065bf46759de","c173bbe534e34f2c913303b17e531f51","413cba7de78042e0abb8e1db7310a3f1","0d26354dbb984e1ab3d8f6a57335b299","199c6f7cc2454aa586225fdff35941a2","e4cb66087d0d4034ab4fb18a92e20b27","392eec678d4743cd89d3b086d4a26d9b","2af7fe02e41248b79f96a3d14defd792","c7ef8f52378e4b88bc9a7293e0b7d029","b066dc3965604a87b9425416610c985a","2bf06c70392945d89397e305f34e8941","fc848de3566c45e0a11553b1a2802962","b1800d11e79f4b19accd754eb662e1c7","d9cd50f271d446949931d12758533f5c","17b42d459852498faa24f54805fa388f","8db76b02ef6f41d6952617d35322a9fa","f09ba207a15b4ff49bbee44df2625119","441827fd89b5476faad6fe15a2686518","68d09183b2914fe0b6f2bb7455d27367","7b975f07f6f74905833594eacc1f4aac","0950891e622e43afa5bb7446f0e03b1d","95d9c1e25cb8421cbb24fb28b97a5f20","e8a3eab41cc047e6bacb4c63d31da863","373be5b835bb4014bd1c6209f47ecdf3","5747b0afc1cc4b5eaa5d1ea1f4d13930","3a3269c0a32f4c9e92e52f6445a854a5","2a37b83c11bf4b95b3bb07defacc48ed","26c6f0b90cb34505829af754e3093355","05cab35f67984e5e8910d639e739026c","d204f9dbb73d4d9c8f41daa60ad35b4e","d3eb79b24cff481ea79c70355a7ed2bd","d62bb2b12d52406880dd6907d11a317b","be18da5b56434324aa006b50f9ae77a1","594c4ee611f64ef3b7f8640fea57f0ed","d75509aff37e48c6bea9b21b5318effa","1115f99671eb45358c51aa7b9b6d4938","561da56b5ee0485eaea2dcaed98b6541"]},"id":"GzfkQLeIiOoi","outputId":"313b2589-118b-4a70-a4c6-a14329f72c27","execution":{"iopub.status.busy":"2024-04-18T22:06:09.421528Z","iopub.execute_input":"2024-04-18T22:06:09.422334Z","iopub.status.idle":"2024-04-18T22:06:13.079576Z","shell.execute_reply.started":"2024-04-18T22:06:09.422305Z","shell.execute_reply":"2024-04-18T22:06:13.078689Z"},"trusted":true},"execution_count":3,"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/1.21k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bceb22714e1e42dc89d0abb6d3f5e375"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/242M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f57a3cdf18d8431eab8b114719f977af"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5bba1260498847069746c704f973215f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/2.32k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ea6231bbf6dd42109a76c63da36f89c5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"253af51a73a247b28f8879c8c7910cdc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.39M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d2a5e5052bf340dc8d6247970f060c6f"}},"metadata":{}},{"name":"stderr","text":"You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\nSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","output_type":"stream"},{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"T5ForConditionalGeneration(\n  (shared): Embedding(32128, 512)\n  (encoder): T5Stack(\n    (embed_tokens): Embedding(32128, 512)\n    (block): ModuleList(\n      (0): T5Block(\n        (layer): ModuleList(\n          (0): T5LayerSelfAttention(\n            (SelfAttention): T5Attention(\n              (q): Linear(in_features=512, out_features=512, bias=False)\n              (k): Linear(in_features=512, out_features=512, bias=False)\n              (v): Linear(in_features=512, out_features=512, bias=False)\n              (o): Linear(in_features=512, out_features=512, bias=False)\n              (relative_attention_bias): Embedding(32, 8)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (1): T5LayerFF(\n            (DenseReluDense): T5DenseActDense(\n              (wi): Linear(in_features=512, out_features=2048, bias=False)\n              (wo): Linear(in_features=2048, out_features=512, bias=False)\n              (dropout): Dropout(p=0.1, inplace=False)\n              (act): ReLU()\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n      (1-5): 5 x T5Block(\n        (layer): ModuleList(\n          (0): T5LayerSelfAttention(\n            (SelfAttention): T5Attention(\n              (q): Linear(in_features=512, out_features=512, bias=False)\n              (k): Linear(in_features=512, out_features=512, bias=False)\n              (v): Linear(in_features=512, out_features=512, bias=False)\n              (o): Linear(in_features=512, out_features=512, bias=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (1): T5LayerFF(\n            (DenseReluDense): T5DenseActDense(\n              (wi): Linear(in_features=512, out_features=2048, bias=False)\n              (wo): Linear(in_features=2048, out_features=512, bias=False)\n              (dropout): Dropout(p=0.1, inplace=False)\n              (act): ReLU()\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n    )\n    (final_layer_norm): T5LayerNorm()\n    (dropout): Dropout(p=0.1, inplace=False)\n  )\n  (decoder): T5Stack(\n    (embed_tokens): Embedding(32128, 512)\n    (block): ModuleList(\n      (0): T5Block(\n        (layer): ModuleList(\n          (0): T5LayerSelfAttention(\n            (SelfAttention): T5Attention(\n              (q): Linear(in_features=512, out_features=512, bias=False)\n              (k): Linear(in_features=512, out_features=512, bias=False)\n              (v): Linear(in_features=512, out_features=512, bias=False)\n              (o): Linear(in_features=512, out_features=512, bias=False)\n              (relative_attention_bias): Embedding(32, 8)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (1): T5LayerCrossAttention(\n            (EncDecAttention): T5Attention(\n              (q): Linear(in_features=512, out_features=512, bias=False)\n              (k): Linear(in_features=512, out_features=512, bias=False)\n              (v): Linear(in_features=512, out_features=512, bias=False)\n              (o): Linear(in_features=512, out_features=512, bias=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (2): T5LayerFF(\n            (DenseReluDense): T5DenseActDense(\n              (wi): Linear(in_features=512, out_features=2048, bias=False)\n              (wo): Linear(in_features=2048, out_features=512, bias=False)\n              (dropout): Dropout(p=0.1, inplace=False)\n              (act): ReLU()\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n      (1-5): 5 x T5Block(\n        (layer): ModuleList(\n          (0): T5LayerSelfAttention(\n            (SelfAttention): T5Attention(\n              (q): Linear(in_features=512, out_features=512, bias=False)\n              (k): Linear(in_features=512, out_features=512, bias=False)\n              (v): Linear(in_features=512, out_features=512, bias=False)\n              (o): Linear(in_features=512, out_features=512, bias=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (1): T5LayerCrossAttention(\n            (EncDecAttention): T5Attention(\n              (q): Linear(in_features=512, out_features=512, bias=False)\n              (k): Linear(in_features=512, out_features=512, bias=False)\n              (v): Linear(in_features=512, out_features=512, bias=False)\n              (o): Linear(in_features=512, out_features=512, bias=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (2): T5LayerFF(\n            (DenseReluDense): T5DenseActDense(\n              (wi): Linear(in_features=512, out_features=2048, bias=False)\n              (wo): Linear(in_features=2048, out_features=512, bias=False)\n              (dropout): Dropout(p=0.1, inplace=False)\n              (act): ReLU()\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n    )\n    (final_layer_norm): T5LayerNorm()\n    (dropout): Dropout(p=0.1, inplace=False)\n  )\n  (lm_head): Linear(in_features=512, out_features=32128, bias=False)\n)"},"metadata":{}}]},{"cell_type":"code","source":"class cqaDataset(Dataset): # Writing my own Dataset class for fast loading\n  def __init__(self, split=\"train\"):\n    # self.tokenizer = tokenizer\n    self.dataset = load_dataset(\"McGill-NLP/TopiOCQA\")[split]\n    self.__buildData()\n\n  def __buildData(self):\n    self.context = []\n    self.questions = []\n    self.answers = []\n\n    for row in self.dataset:\n      self.questions.append(row[\"Question\"])\n      self.answers.append(row[\"Answer\"])\n      if len(row[\"Context\"])>0:\n        ctx=\"\"\n        for c in row[\"Context\"]:\n          ctx+=c\n          ctx+=\";\"\n        self.context.append(ctx[:-1])\n      else:\n        self.context.append(\"\")\n\n  def __len__(self):\n    return len(self.questions)\n\n  def __getitem__(self, idx):\n    return self.context[idx], self.questions[idx], self.answers[idx]","metadata":{"id":"c05XcBVPpI7h","execution":{"iopub.status.busy":"2024-04-18T22:06:19.568573Z","iopub.execute_input":"2024-04-18T22:06:19.568942Z","iopub.status.idle":"2024-04-18T22:06:19.577398Z","shell.execute_reply.started":"2024-04-18T22:06:19.568914Z","shell.execute_reply":"2024-04-18T22:06:19.576435Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"# CONFIGS ....\nmax_inp_len=512\nlr = 10**-4\nepochs = 5\nwd = 0.0\ntrn_bs = 16\ntst_bs = 64\noptim_type=\"adam\"\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\ndevice","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"_XR67LXly3jl","outputId":"c2fdfef0-8122-4dde-d3c6-8712ef070724","execution":{"iopub.status.busy":"2024-04-18T22:06:21.680278Z","iopub.execute_input":"2024-04-18T22:06:21.680653Z","iopub.status.idle":"2024-04-18T22:06:21.711157Z","shell.execute_reply.started":"2024-04-18T22:06:21.680608Z","shell.execute_reply":"2024-04-18T22:06:21.710082Z"},"trusted":true},"execution_count":5,"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"'cuda'"},"metadata":{}}]},{"cell_type":"code","source":"train_set = cqaDataset()\ntrain_loader = DataLoader(train_set, batch_size=trn_bs, shuffle=True)\ndev_set = cqaDataset(split=\"validation\")\ndev_loader = DataLoader(dev_set, batch_size=tst_bs, shuffle=False)","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":145,"referenced_widgets":["82403c3bdfd944b8820b1840e2c66fd5","567468a3a22b45f8b50a1cdbce8438c5","48808ca04ebc4b20b9f9224bf5956c46","6f2fd1b9283c49f1a23b9b12672e723a","daee0456d83049daa8ee85d3aa11c885","782e21dc4b4e46fab69bd9a4b11ecc2b","a6a26f45114e48828d764063faf59fa8","7d23e52549434814b97472564a739bf2","ec8196f735b7410286e0973488b7d563","f84a73def8a64d6687ec262dafc78916","1db1a999b2a94f2ab49bf477131cdf98","8323b0a6e96b441ca8e3f56648983f39","7e816eff335241499fdca241b1574618","98aaf3bef7f4411c88f9111b40318222","9af5ee7c99734de99339b5892bcabeee","180acc32bcf844afa003b0a192ab886b","4385d5e66db749efa38228696f867aa0","fad884d050a243b5a147622eef7d12f4","8946dab706404516bfe41cf0fc5a36b2","55cea314c38b4db4a608e5669ec22702","53f9a73cfd714ed886ad2c84dba62ef0","573f0c787b1a4164a7e17e2f1a0ed36e","6f1443064e46402285fdadb57df768b3","e310b9a0a8ef4ab3a3f15a4b376a9477","6fd0f9d0aeb148fd881508c4ea24b4e5","f6637a0ba02848d9bdf43af4b16ead4d","8aa4b66d5a4142efb76e49d2ae87bb49","0cf72d354d5648d6823cfca1b85c5697","945388369ffa48d3ad64422ce061774b","1299e4509c2a4e0aa8d9d66c65898c4d","d7d0d24f15034afa8b0a2f4960ce64e4","cc4942dae23149e8a2fb9e22c826f673","31e7dfe0af9a4fc681f92008849e0f31","387e6aa757e2415daeb073918f36f1ae","88286156df064f3cb18df70c0564b140","d63a13bc7bf242189d531f376e52f8c0","09ca0b2e53684e4688bae2b50c4f2450","69d1a488e5da477d95e2242a0f217b61","fbbec830c1af40a28a16896d0ab17cb8","a71936bb55134b238d7f34397bdc41ba","b73c089b65eb4bcd9513b5c8e68159b5","48753c893d9d40e095fa7415a2606354","a99500f6740a43008999530b6bc9dfdf","d936c910bc3544068bbbda821404a6a3"]},"id":"li22iItZt9X2","outputId":"9e391fcd-81f4-439a-af10-75b3a33c8378","execution":{"iopub.status.busy":"2024-04-18T22:06:22.813197Z","iopub.execute_input":"2024-04-18T22:06:22.813560Z","iopub.status.idle":"2024-04-18T22:06:39.429367Z","shell.execute_reply.started":"2024-04-18T22:06:22.813530Z","shell.execute_reply":"2024-04-18T22:06:39.428545Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stderr","text":"Downloading data: 100%|██████████| 28.9M/28.9M [00:01<00:00, 19.8MB/s]\nDownloading data: 100%|██████████| 2.64M/2.64M [00:00<00:00, 14.3MB/s]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/45450 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ea3d1ddab1564e3fb8428b9a9b50ae3b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating validation split:   0%|          | 0/2514 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b192a6123fe74e779617c24291463c8c"}},"metadata":{}}]},{"cell_type":"code","source":"def evaluate(preds, gold):\n\n  def f1_score(p,g):\n    common = Counter(p) & Counter(g)\n    num_same = sum(common.values())\n    if num_same == 0:\n      return 0\n    precision = 1.0 * num_same / len(p)\n    recall = 1.0 * num_same / len(g)\n    f1 = (2 * precision * recall) / (precision + recall)\n    return f1\n\n  def exact_match_calc(p,g):\n    if len(g) == len(p):\n      if all(token1 == token2 for token1, token2 in zip(g,p)):\n        return 1\n    return 0\n\n  f1 = exact_match = 0\n  for gld, pred in tqdm(zip(gold, preds)):\n    # Remove pad token\n    tokens_to_remove = {\n        tokenizer.pad_token_id,\n        tokenizer.eos_token_id,\n        tokenizer.bos_token_id,\n        tokenizer.cls_token_id,\n        tokenizer.sep_token_id,\n        tokenizer.mask_token_id\n    }\n    pred = list(filter(lambda token: token not in tokens_to_remove, pred))\n    gld = list(filter(lambda token: token not in tokens_to_remove, gld))\n    f1 += f1_score(p=pred, g=gld)\n    exact_match += exact_match_calc(p=pred, g=gld)\n  return 100*f1/len(preds), 100*exact_match/len(preds)","metadata":{"id":"VmhhJgUW90LS","execution":{"iopub.status.busy":"2024-04-18T22:06:44.367540Z","iopub.execute_input":"2024-04-18T22:06:44.367911Z","iopub.status.idle":"2024-04-18T22:06:44.378062Z","shell.execute_reply.started":"2024-04-18T22:06:44.367883Z","shell.execute_reply":"2024-04-18T22:06:44.376720Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"class Model(nn.Module): # Wrapper class for putting everything needed to train the t5 model\n  def __init__(self):\n    super().__init__()\n    self.t5=t5model\n    self.__makeOptim()\n    self.to(device)\n    self.tot_params=-1\n    self.checkpoint_dir = \"/kaggle/working/\"\n    self.model_name = model_name\n    self.tokenizer=tokenizer\n\n  def forward(self, data):\n    input_ids, attention_mask, encoded_targets = self.__makeData(data=data)\n    return self.t5(input_ids=input_ids, attention_mask=attention_mask, labels=encoded_targets)\n\n  def gen(self, data):\n    input_ids, attention_mask, encoded_targets = self.__makeData(data=data,to_train=False)\n    return self.t5.generate(input_ids=input_ids, attention_mask=attention_mask), encoded_targets\n\n  def __makeData(self, data, to_train=True):\n    inputs = list(map(lambda tuple: f\"question:{tuple[0]}  context:{tuple[1]}\", zip(data[1],data[0])))\n    encoded_inputs = self.tokenizer(\n                            inputs,\n                            padding=\"longest\",\n                            max_length=max_inp_len,\n                            truncation=True,\n                            return_tensors=\"pt\",\n                        )\n    encoded_targets = self.tokenizer(\n                            list(data[2]),\n                            padding=\"longest\",\n                            max_length=max_inp_len,\n                            truncation=True,\n                            return_tensors=\"pt\",\n                        )\n\n    input_ids, attention_mask = encoded_inputs.input_ids, encoded_inputs.attention_mask\n    encoded_targets = encoded_targets.input_ids\n    if to_train:\n      encoded_targets[encoded_targets == self.tokenizer.pad_token_id] = -100\n\n    input_ids = input_ids.to(device)\n    encoded_targets = encoded_targets.to(device)\n    attention_mask = attention_mask.to(device)\n\n    return input_ids, attention_mask, encoded_targets\n\n  def __makeOptim(self):\n    optim_grp_params = [\n        {\n            \"params\": [p for n,p in self.named_parameters() if \"bias\" not in n],\n            \"weight_decay\": wd,\n        },\n        {\n            \"params\": [p for n,p in self.named_parameters() if \"bias\" in n],\n            \"weight_decay\": 0.0,\n        }\n    ]\n    if optim_type==\"adam\":\n      self.optimizer = optim.Adam(optim_grp_params, lr=lr)\n    elif optim_type==\"adam-amsgrad\":\n      self.optimizer = optim.Adam(optim_grp_params, lr=lr, amsgrad=True)\n    elif optim_type==\"adamw\":\n      self.optimizer = optim.AdamW(optim_grp_params, lr=lr)\n    elif optim_type==\"rmsprop\":\n      self.optimizer = optim.RMSprop(optim_grp_params, lr=lr)\n    elif optim_type==\"sgd\":\n      self.optimizer = optim.SGD(optim_grp_params, lr=lr)\n    elif optim_type==\"sgd-nesterov\":\n      self.optimizer = optim.SGD(optim_grp_params, lr=lr, nesterov=True, momentum=0.9)\n    else:\n      raise NotImplementedError(\"optim_type not implemented yet :/\")\n\n  def save_checkpoint(self,epoch):\n    torch.save(self.state_dict(), os.path.join(self.checkpoint_dir,\"contextualT5_\"+str(epoch)))\n\n  def save_best(self):\n    torch.save(self.state_dict(), os.path.join(self.checkpoint_dir,\"contextualT5_best\"))\n\n  def load_best(self):\n    self.load_state_dict(torch.load(os.path.join(self.checkpoint_dir,\"contextualT5_best\"), map_location=device))\n\n  # def load_checkpoint(self, epoch):\n  #   self.load_state_dict(torch.load(os.path.join(self.checkpoint_dir,\"contextualT5_\"+str(epoch)), map_location=device))\n\n  def calc_params(self, ret=False, verbose=True):\n    if self.tot_params==-1:\n      self.tot_params=0\n      for p in self.optimizer.param_groups:\n        for k,v in p.items():\n          if k==\"params\":\n            for x in v:\n              temp_sr=1\n              for i in range(len(x.shape)):\n                temp_sr*=x.shape[i]\n              self.tot_params += temp_sr\n    if verbose:\n      print(\"Total number of trainable parameters in \"+self.model_name+\" = \"+str(self.tot_params))\n    if ret:\n      return self.tot_params","metadata":{"id":"j_yCEKvWzkG5","execution":{"iopub.status.busy":"2024-04-18T22:06:45.233405Z","iopub.execute_input":"2024-04-18T22:06:45.233748Z","iopub.status.idle":"2024-04-18T22:06:45.283803Z","shell.execute_reply.started":"2024-04-18T22:06:45.233724Z","shell.execute_reply":"2024-04-18T22:06:45.282662Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"model = Model()\nmodel.calc_params()","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hqC_Wvij2L0P","outputId":"61715d91-8281-45f0-c07b-06f15e55ec83","execution":{"iopub.status.busy":"2024-04-18T22:06:46.498426Z","iopub.execute_input":"2024-04-18T22:06:46.498828Z","iopub.status.idle":"2024-04-18T22:06:46.748740Z","shell.execute_reply.started":"2024-04-18T22:06:46.498798Z","shell.execute_reply":"2024-04-18T22:06:46.747778Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"Total number of trainable parameters in t5-small = 60506624\n","output_type":"stream"}]},{"cell_type":"code","source":"model.eval()\npreds = []\nlabels_encoded = []\nwith torch.no_grad():\n    for data in tqdm(dev_loader):\n        outs,tars = model.gen(data)\n        preds += outs.tolist()\n        labels_encoded += tars.tolist()\nf1, exact_match = evaluate(preds=preds, gold=labels_encoded)\nprint(f\"Benchmark Scores --> Validation F1 = {f1:.2f}, EM = {exact_match:.2f}\")","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"n4KvbBMnGmkK","outputId":"e9f96741-082c-45db-ea48-ceae013b2083","execution":{"iopub.status.busy":"2024-04-18T22:06:49.833473Z","iopub.execute_input":"2024-04-18T22:06:49.833936Z","iopub.status.idle":"2024-04-18T22:07:16.869920Z","shell.execute_reply.started":"2024-04-18T22:06:49.833901Z","shell.execute_reply":"2024-04-18T22:07:16.869063Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stderr","text":"100%|██████████| 40/40 [00:26<00:00,  1.48it/s]\n2514it [00:00, 33791.32it/s]","output_type":"stream"},{"name":"stdout","text":"Benchmark Scores --> Validation F1 = 6.02, EM = 0.36\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}]},{"cell_type":"code","source":"# Takes around 35 mins per epoch on colab's T4 GPU & ~16 mins per epoch on Kaggle's P100 GPU ....\nf1_old = f1 # initializing w/o finetuned metric for starters \nfor epoch in range(epochs):\n\n  model.train()\n  avg_loss = 0.0\n  for data in tqdm(train_loader):\n      model.optimizer.zero_grad()\n      outs = model(data)\n      loss = outs.loss\n      loss.backward()\n      model.optimizer.step()\n      avg_loss += (trn_bs*loss.item())\n  print(\"Epoch: \"+str(epoch)+\"| Avg Loss: \"+str(avg_loss/len(train_set)))\n  model.save_checkpoint(epoch)\n\n  model.eval()\n  preds = []\n  labels_encoded = []\n  with torch.no_grad():\n      for data in tqdm(dev_loader):\n          outs,tars = model.gen(data)\n          preds += outs.tolist()\n          labels_encoded += tars.tolist()\n  f1, exact_match = evaluate(preds=preds, gold=labels_encoded)\n  print(f\"Epoch = {epoch}, Validation F1 = {f1:.2f}, EM = {exact_match:.2f}\")\n  if f1 > f1_old :\n      model.save_best()\n      f1_old = f1","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":490},"id":"geSMhxLH13vC","outputId":"688e8e62-04b5-4d51-f78a-9d89cf7f0362","execution":{"iopub.status.busy":"2024-04-18T22:08:00.936623Z","iopub.execute_input":"2024-04-18T22:08:00.937050Z","iopub.status.idle":"2024-04-18T23:21:40.531233Z","shell.execute_reply.started":"2024-04-18T22:08:00.937022Z","shell.execute_reply":"2024-04-18T23:21:40.530210Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stderr","text":"100%|██████████| 2841/2841 [14:21<00:00,  3.30it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch: 0| Avg Loss: 3.757974936272314\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 40/40 [00:23<00:00,  1.72it/s]\n2514it [00:00, 31332.77it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch = 0, Validation F1 = 16.15, EM = 0.95\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 2841/2841 [14:20<00:00,  3.30it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch: 1| Avg Loss: 3.5905152245952743\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 40/40 [00:23<00:00,  1.73it/s]\n2514it [00:00, 32848.75it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch = 1, Validation F1 = 16.86, EM = 1.47\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 2841/2841 [14:19<00:00,  3.31it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch: 2| Avg Loss: 3.5021842626691257\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 40/40 [00:23<00:00,  1.73it/s]\n2514it [00:00, 33165.84it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch = 2, Validation F1 = 17.11, EM = 1.79\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 2841/2841 [14:19<00:00,  3.31it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch: 3| Avg Loss: 3.4355419569361723\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 40/40 [00:23<00:00,  1.72it/s]\n2514it [00:00, 33643.18it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch = 3, Validation F1 = 16.77, EM = 1.55\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 2841/2841 [14:18<00:00,  3.31it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch: 4| Avg Loss: 3.3768131288836893\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 40/40 [00:22<00:00,  1.74it/s]\n2514it [00:00, 32594.49it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch = 4, Validation F1 = 17.39, EM = 2.47\n","output_type":"stream"}]},{"cell_type":"code","source":"# Loading the best model ...\nmodel.load_best()\nmodel.eval()\npreds = []\nlabels_encoded = []\nwith torch.no_grad():\n    for data in tqdm(dev_loader):\n        outs,tars = model.gen(data)\n        preds += outs.tolist()\n        labels_encoded += tars.tolist()\nf1, exact_match = evaluate(preds=preds, gold=labels_encoded)\nprint(f\"Best Model Scores --> Validation F1 = {f1:.2f}, EM = {exact_match:.2f}\")","metadata":{"id":"sXAGLMHfM1F6","execution":{"iopub.status.busy":"2024-04-18T23:21:40.533085Z","iopub.execute_input":"2024-04-18T23:21:40.533441Z","iopub.status.idle":"2024-04-18T23:22:03.797767Z","shell.execute_reply.started":"2024-04-18T23:21:40.533411Z","shell.execute_reply":"2024-04-18T23:22:03.796594Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stderr","text":"100%|██████████| 40/40 [00:22<00:00,  1.74it/s]\n2514it [00:00, 32435.57it/s]","output_type":"stream"},{"name":"stdout","text":"Best Model Scores --> Validation F1 = 17.39, EM = 2.47\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}]},{"cell_type":"code","source":"'''\n  Remarks:\n  ** For the purpose of this assignment, I've just used a few basic metrics like F1-score and Exact Match. However, more sophisticated eval metrics like ROGUE, BLEU, etc. can be used for better model quality evaluation.\n  ** Future scope of work includes experimenting with better trigger words than just using \"context\" & \"question\", maybe using the rationale as well\n  ** As compared to benchmark metrics (w/o finetuning model), we can see a significant improvement (~189% improvement in the F1 score and ~586% improvement in the Exact Match Score) in both the metrics with just 5 epochs of SFT!\n'''","metadata":{"id":"ywKHoUV3LbTu"},"execution_count":null,"outputs":[]}]}